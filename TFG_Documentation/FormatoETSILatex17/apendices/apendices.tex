% !TEX root =../LibroTipoETSI.tex



%APENDICE A
\chapter{Codes for Designing Transformer, Informer, and Autoformer Models}\LABAPEN{ApA}
{This appendix provides the Python code utilized for designing and implementing various models discussed in this work, specifically Transformer, Informer, and Autoformer. The included scripts cover different stages of the model development process, including data loading and preprocessing, model definition, training, and evaluation.}
%%%%%%%%%%%%%%%%%
\section{Orchestrator Code}

The main.py script serves as the central execution point for time series forecasting using different model architectures: Transformer, Informer, and Autoformer. The script is structured in a modular manner to facilitate a clear understanding of each step in the forecasting process. Initially, it loads and preprocesses the dataset tailored to the required frequency and prediction length. The script then defines the model architecture based on the chosen model variant and configures the training and backtesting data loaders. The model is trained over a specified number of epochs, leveraging the training data to learn and improve its forecasting capabilities. Once trained, the model's forecasting performance is evaluated on unseen data, and relevant metrics are computed to assess accuracy. Finally, the script generates plots to visually compare the forecasted values with the actual data, offering insights into the model's predictive performance across different variables.

\begin{lstlisting}[language=Python, caption={Code for entry point and orchestration of the time series forecasting pipeline}, breaklines=true, label=code1]

    # %% IMPORT LIBRARIES

    from T1_Load_Dataset import load_and_preprocess_dataset
    from T2_Define_Model import define_my_model
    from T3_Create_DataLoader import create_train_dataloader, create_backtest_dataloader
    from T4_Train_Model import train_model
    from T5_Evaluate_Model import forecasting, see_metrics, plot
    
    # %% DEFINE PARAMETERS
    
    # Select the model variant to use: "Transformer", "Informer", "Autoformer"
    model_variant = "Autoformer"
    
    # Define the frequency of the data sampling and the length of the forecast horizon
    freq = "7min52s"
    prediction_length = 366
    
    # Set the number of epochs for model training
    num_of_epochs = 20
    
    # %% LOAD, SPLIT AND PREPROCESS DATASET
    
    # Load and preprocess the dataset according to the specified frequency and prediction length.
    # The function returns the training and test datasets, the number of variables in the dataset,
    # and the test dataset in its original format for evaluation.
    (multi_variate_train_dataset,
     multi_variate_test_dataset,
     num_of_variates,
     test_dataset) = load_and_preprocess_dataset(freq, prediction_length)
    
    # %% DEFINE THE MODEL
    
    # Define the model architecture based on the selected variant (e.g., Autoformer).
    # The model configuration depends on the number of variables, training data, frequency,
    # and prediction length.
    model = define_my_model(num_of_variates, multi_variate_train_dataset, model_variant, freq, prediction_length)
    
    # %% CREATE DATA LOADERS
    
    # Create a data loader for the training dataset, specifying the batch size and number of batches per epoch.
    # The data loader enables efficient loading of data during training.
    train_dataloader = create_train_dataloader(
        config=model.config,
        freq=freq,
        data=multi_variate_train_dataset,
        batch_size=32,
        num_batches_per_epoch=100,
        num_workers=2,
    )
    
    # Create a data loader for the backtesting dataset to evaluate the model on unseen data.
    test_dataloader = create_backtest_dataloader(
        config=model.config,
        freq=freq,
        data=multi_variate_test_dataset,
        batch_size=32,
    )
    
    # %% TRAIN THE MODEL
    
    # Train the model using the training data loader for the specified number of epochs.
    # The loss function used during training is determined by the model variant.
    train_model(num_of_epochs, model, train_dataloader, model_variant + "_Loss")
    
    # %% FORECASTING AND EVALUATION
    
    # Generate forecasts using the trained model and the test data loader.
    forecasts = forecasting(model, test_dataloader)
    
    # Evaluate the model's performance by calculating relevant metrics and saving them to a file.
    see_metrics(forecasts, test_dataset, prediction_length, freq, "metrics.txt", model_variant + "_Metrics")
    
    # Plot the forecasted and actual values for different variables to visually assess the model's performance.
    plot(forecasts, 0, 0, multi_variate_test_dataset, freq, prediction_length, model_variant + "_Temperature")
    plot(forecasts, 0, 1, multi_variate_test_dataset, freq, prediction_length, model_variant + "_Relative_humidity")
    plot(forecasts, 0, 2, multi_variate_test_dataset, freq, prediction_length, model_variant + "_Light")
    plot(forecasts, 0, 3, multi_variate_test_dataset, freq, prediction_length, model_variant + "_Soil_Temperature")
    plot(forecasts, 0, 4, multi_variate_test_dataset, freq, prediction_length, model_variant + "_Permittivity")
    plot(forecasts, 0, 5, multi_variate_test_dataset, freq, prediction_length, model_variant + "_Electroconductivity")
    plot(forecasts, 0, 6, multi_variate_test_dataset, freq, prediction_length, model_variant + "_Diameter")
    

\end{lstlisting}

\section{Data Preprocessing Code}

The \texttt{T1\_Load\_Dataset.py} script is responsible for the comprehensive loading and preprocessing of time series data, preparing it for use with forecasting models such as Transformers, Informers, and Autoformers. This script consists of two primary functions: \texttt{load\_and\_preprocess\_dataset} and \texttt{load\_my\_own\_dataset}.

The \texttt{load\_and\_preprocess\_dataset} function coordinates the conversion of raw data into a format suitable for multivariate time series forecasting. It begins by calling \texttt{load\_my\_own\_dataset} to read and preprocess the data. Subsequent transformations are applied, including handling missing values, detrending, and smoothing. After normalization, the data is split into training, validation, and test sets. These datasets are then reformatted using GluonTS tools, which are essential for time series modeling.

The \texttt{load\_my\_own\_dataset} function performs specific tasks related to data loading and initial preprocessing. It reads the dataset from a CSV file, selects relevant features, removes missing values, and applies trend removal techniques. It also normalizes the data and splits it into training, validation, and test subsets. Additionally, this function includes visualization steps to evaluate the quality of the data and the effects of preprocessing.



\begin{lstlisting}[language=Python, caption={Code for loading and preprocessing the time series data}, breaklines=true, label=code2]

    # %% LIBRARIES

    from gluonts.dataset.multivariate_grouper import MultivariateGrouper
    from functools import partial, lru_cache
    import pandas as pd
    import matplotlib.pyplot as plt
    import os
    from scipy.signal import savgol_filter
    from datasets import Dataset
    from sklearn.preprocessing import MinMaxScaler
    import matplotlib.dates as mdates
    import numpy as np

    # %% LOAD AND PREPROCESS DATASET FUNCTION

    def load_and_preprocess_dataset(freq, prediction_length):
    """
    Loads and preprocesses the dataset, then converts it into a multivariate time series
    suitable for training and testing forecasting models.

    Parameters:
    - freq (str): The frequency of data points in the dataset (e.g., '7min52s').
    - prediction_length (int): The number of time steps to predict.

    Returns:
    - tuple: Contains the multivariate training dataset, multivariate testing dataset,
                the number of variables, and the original test dataset.
    """

    # Load custom dataset using a helper function
    validationtest_dataset, test_dataset, train_dataset = load_my_own_dataset(prediction_length)

    # Cache for converting date to pandas period
    @lru_cache(maxsize=10_000)
    def convert_to_pandas_period(date, freq):
        return pd.Period(date, freq)

    # Transform function for dataset start field
    def transform_start_field(batch, freq):
        batch["start"] = [convert_to_pandas_period(date, freq) for date in batch["start"]]
        return batch

    # Apply the transformation to datasets
    train_dataset.set_transform(partial(transform_start_field, freq=freq))
    test_dataset.set_transform(partial(transform_start_field, freq=freq))

    # Determine the number of variables in the dataset
    num_of_variates = len(train_dataset)

    # Group datasets for multivariate time series forecasting
    train_grouper = MultivariateGrouper(max_target_dim=num_of_variates)
    test_grouper = MultivariateGrouper(max_target_dim=num_of_variates, num_test_dates=len(test_dataset) // num_of_variates)

    # Transform datasets into multivariate format
    multi_variate_train_dataset = train_grouper(train_dataset)
    multi_variate_test_dataset = test_grouper(test_dataset)

    return multi_variate_train_dataset, multi_variate_test_dataset, num_of_variates, test_dataset

    # %% LOAD CUSTOM DATASET FROM CSV FUNCTION

    def load_my_own_dataset(prediction_length):
    """
    Loads and preprocesses the custom dataset from a CSV file, normalizes the data, 
    and splits it into training, validation, and testing datasets.

    Parameters:
    - prediction_length (int): The number of time steps to predict.

    Returns:
    - tuple: Contains the validation, test, and training datasets in GluonTS format.
    """

    # Construct the file path to the dataset
    current_path = os.path.dirname(os.path.abspath(__file__))
    directory_name = 'Create_2024_dataset'
    file_name = 'Clay_2.csv'
    file_path = os.path.join(current_path, directory_name, file_name)

    # Load the dataset into a pandas DataFrame
    df = pd.read_csv(file_path, sep=";")

    # Select relevant variables for forecasting
    data = df[['Temperature', 'Relative_humidity', 'Light', 'Soil_temperature',
                'Permittivity', 'Electroconductivity', 'Diameter']]

    # Remove rows with NaN values
    index_nan = data[data.isna().any(axis=1)].index
    data = data.dropna()

    # Detrend data by removing the rolling mean
    window_value = 100
    for col in ['Diameter']:
        data.loc[:, col] = data[col] - data[col].rolling(window=window_value).mean()

    # Drop rows with NaN values created by detrending
    data = data.dropna()

    # Visualize the distribution of eliminated data points
    plt.figure(figsize=(10, 2))
    plt.plot(index_nan, np.ones_like(index_nan), 'ro', markersize=2)
    plt.title(f'Nan index distribution. \n Number of eliminated measurements: {len(index_nan)}')
    plt.xlabel('Index')
    plt.ylabel('Frequency')
    plt.yticks([])
    plt.grid(True)
    plt.xlim(0, len(df))
    plt.show()

    # Smooth the data using Savitzky-Golay filter
    for col in data.columns:
        data[col] = savgol_filter(data[col], 11, 2)

    # Convert date column to datetime and calculate the sampling period
    dates = pd.to_datetime(df['Date'].drop(index_nan))
    intervals = dates.diff()
    sampling_period = intervals.mean()
    print("\nAverage sampling period:\n", sampling_period)

    # Normalize the data using Min-Max Scaler
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(data)
    data[data.columns] = scaled_data

    # Split data into training, validation, and test sets
    data_test = data
    data_validation = data.iloc[:-prediction_length]
    data_train = data.iloc[:-2 * prediction_length]

    # Prepare datasets for GluonTS
    dict_validation = {'start': [], 'target': [], 'feat_static_cat': [], 'feat_dynamic_real': [], 'item_id': []}
    dict_test = {'start': [], 'target': [], 'feat_static_cat': [], 'feat_dynamic_real': [], 'item_id': []}
    dict_train = {'start': [], 'target': [], 'feat_static_cat': [], 'feat_dynamic_real': [], 'item_id': []}

    # Populate dictionaries with data for GluonTS
    for i in range(1, 8):
        dict_validation['target'].append(data_validation.iloc[:, i-1].values.astype('float32'))
        dict_test['target'].append(data_test.iloc[:, i-1].values.astype('float32'))
        dict_train['target'].append(data_train.iloc[:, i-1].values.astype('float32'))

        for d in [dict_validation, dict_test, dict_train]:
            d['start'].append(pd.Timestamp('2022-01-01 13:14:26'))
            d['feat_static_cat'].append(i)
            d['feat_dynamic_real'].append(None)
            d['item_id'].append(f'T{i}')

    # Convert dictionaries to GluonTS datasets
    dataset_validation = Dataset.from_pandas(pd.DataFrame(dict_validation))
    dataset_test = Dataset.from_pandas(pd.DataFrame(dict_test))
    dataset_train = Dataset.from_pandas(pd.DataFrame(dict_train))

    # Visualize training and test data
    plot_dataset(train_dataset=dataset_train, test_dataset=dataset_test, data=data, prediction_length=prediction_length)

    return dataset_validation, dataset_test, dataset_train

    # %% HELPER FUNCTION TO PLOT DATASETS

    def plot_dataset(train_dataset, test_dataset, data, prediction_length):
    """
    Plots the training and test datasets to visualize the time series data used for forecasting.

    Parameters:
    - train_dataset (Dataset): The training dataset in GluonTS format.
    - test_dataset (Dataset): The test dataset in GluonTS format.
    - data (pd.DataFrame): The original data used for forecasting.
    - prediction_length (int): The number of time steps to predict.
    """

    start_date = "2022-01-01"
    frequency = '7min52s'

    def generate_dates(start, num_periods, freq):
        return pd.date_range(start=start, periods=num_periods, freq=freq)

    for var in range(7):
        num_periods_train = len(train_dataset[var]["target"])
        num_periods_test = len(test_dataset[var]["target"])

        train_dates = generate_dates(start_date, num_periods_train, frequency)
        test_dates = generate_dates(start_date, num_periods_test, frequency)

        # Plot full data
        plt.figure(figsize=(20, 6.4))
        plt.plot(train_dates, train_dataset[var]["target"], color="blue", label="Train")
        plt.plot(test_dates[-2*prediction_length:], test_dataset[var]["target"][-2*prediction_length:], color="red", label="Test")
        plt.title(data.columns[var].replace('_', ' '))
        plt.legend()
        plt.xticks(rotation=30, ha='right')
        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
        plt.show()

        # Plot zoomed segment
        plt.figure()
        plt.plot(train_dates[-3*prediction_length:], train_dataset[var]["target"][-3*prediction_length:], color="blue", label="Train (zoom)")
        plt.plot(test_dates[-2*prediction_length:], test_dataset[var]["target"][-2*prediction_length:], color="red", label="Test (zoom)")
        plt.title(data.columns[var].replace('_', ' '))
        plt.legend()
        plt.xticks(rotation=30, ha='right')
        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
        plt.show()
    

\end{lstlisting}

\section{Model Definition Code}

The \texttt{define\_my\_model} function is designed to create and configure a time series forecasting model based on the specified variant: "Transformer", "Informer", or "Autoformer". The function takes several parameters, including the number of variates in the time series, the dataset, the chosen model variant, the frequency of the time series data, and the prediction length.

Firstly, the function retrieves default lag values and time features based on the given frequency using GluonTS utilities. It then configures the model according to the selected variant.

If an invalid model variant is provided, the function raises a \texttt{ValueError}. The function returns the configured model instance, ready for training and evaluation. This approach ensures that the model architecture is tailored to the specific needs of the time series data and forecasting task.

\begin{lstlisting}[language=Python, caption={Code for defining the architecture and hyperparameters of the forecasting model}, breaklines=true, label=code3]

    # %% LIBRARIES

    from transformers import InformerConfig, InformerForPrediction
    from transformers import AutoformerConfig, AutoformerForPrediction
    from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction
    from gluonts.time_feature import time_features_from_frequency_str, get_lags_for_frequency
    import pandas as pd
    
    # %% DEFINE MODEL
    
    def define_my_model(num_of_variates, multi_variate_train_dataset, model_variant, freq, prediction_length):
        """
        Defines and returns a time series forecasting model based on the specified variant.
        
        Parameters:
        - num_of_variates (int): Number of variables in the time series data.
        - multi_variate_train_dataset (Dataset): Training dataset in a multivariate format.
        - model_variant (str): Type of model to define. Options are "Transformer", "Informer", or "Autoformer".
        - freq (str): Frequency of the time series data (e.g., '7min52s').
        - prediction_length (int): Length of the prediction horizon.
        
        Returns:
        - model (Model): Configured model instance for forecasting.
        """
        
        # Get default lags and time features based on the frequency
        lags_sequence = get_lags_for_frequency(freq)
        time_features = time_features_from_frequency_str(freq)
    
        # Define model configuration based on the specified model variant
        if model_variant == "Transformer":
            config = TimeSeriesTransformerConfig(
                input_size=num_of_variates,
                prediction_length=prediction_length,
                context_length=prediction_length * 2,
                lags_sequence=[
                    1, 2, 3, 4, 5, 183 + 1, 183 + 2, 183 + 3, 183 + 4, 183 + 5,
                    366 + 1, 366 + 2, 366 + 3, 366 + 4, 366 + 5, 549 + 1, 549 + 2,
                    549 + 3, 549 + 4, 549 + 5
                ],
                num_time_features=len(time_features) + 1,
                dropout=0.2,
                encoder_layers=4,
                decoder_layers=4,
                d_model=32,
            )
            model = TimeSeriesTransformerForPrediction(config)
    
        elif model_variant == "Informer":
            config = InformerConfig(
                input_size=num_of_variates,
                prediction_length=prediction_length,
                context_length=prediction_length * 2,
                lags_sequence=[
                    1, 2, 3, 4, 5, 183 + 1, 183 + 2, 183 + 3, 183 + 4, 183 + 5,
                    366 + 1, 366 + 2, 366 + 3, 366 + 4, 366 + 5, 549 + 1, 549 + 2,
                    549 + 3, 549 + 4, 549 + 5
                ],
                num_time_features=len(time_features) + 1,
                dropout=0.1,
                encoder_layers=4,
                decoder_layers=4,
                d_model=32,
            )
            model = InformerForPrediction(config)
    
        elif model_variant == "Autoformer":
            config = AutoformerConfig(
                input_size=num_of_variates,
                prediction_length=prediction_length,
                context_length=prediction_length * 2,
                lags_sequence=lags_sequence,
                num_time_features=len(time_features) + 1,
                dropout=0.1,
                encoder_layers=6,
                decoder_layers=4,
                d_model=64,
            )
            model = AutoformerForPrediction(config)
    
        else:
            raise ValueError("ERROR: No valid model variant specified. Choose 'Transformer', 'Informer', or 'Autoformer'.")
    
        return model
    

\end{lstlisting}

\section{Data Loader Creation Code}

The provided code defines functions for creating data loaders for training, backtesting, and testing time series models using the GluonTS library. These functions preprocess time series data according to the model's configuration and the specific mode of operation.

\begin{itemize}
    \item \texttt{create\_train\_dataloader}: Prepares and transforms training data by applying necessary transformations and splitting it into batches. It also supports caching and shuffling of data.
    \item \texttt{create\_backtest\_dataloader}: Configures a data loader for backtesting purposes, focusing on validation data.
    \item \texttt{create\_test\_dataloader}: Sets up a data loader for testing, processing data for evaluation on unseen samples.
\end{itemize}

The \texttt{create\_instance\_splitter} function creates an instance splitter based on the mode (train, validation, or test), which partitions the time series data into instances suitable for model training or evaluation. The \texttt{create\_transformation} function generates a transformation pipeline that preprocesses the data, including handling missing values, adding time-based features, and renaming fields to match the expected input format.

These functions ensure that the data is properly formatted and split for model training, validation, and testing, facilitating effective model evaluation and performance monitoring.

\begin{lstlisting}[language=Python, caption={Code for creating data loaders for efficient training and evaluation}, breaklines=true, label=code4]

    # %% LIBRARIES

    from typing import Iterable, Optional
    from transformers import PretrainedConfig
    import torch
    from gluonts.itertools import Cached, Cyclic
    from gluonts.dataset.loader import as_stacked_batches
    from gluonts.time_feature import time_features_from_frequency_str, TimeFeature, get_lags_for_frequency
    from gluonts.dataset.field_names import FieldName
    from gluonts.transform import (
        AddAgeFeature,
        AddObservedValuesIndicator,
        AddTimeFeatures,
        AsNumpyArray,
        Chain,
        ExpectedNumInstanceSampler,
        InstanceSplitter,
        RemoveFields,
        SelectFields,
        SetField,
        TestSplitSampler,
        Transformation,
        ValidationSplitSampler,
        VstackFeatures,
        RenameFields,
    )
    from gluonts.transform.sampler import InstanceSampler
    
    
    def create_train_dataloader(
        config: PretrainedConfig,
        freq: str,
        data,
        batch_size: int,
        num_batches_per_epoch: int,
        shuffle_buffer_length: Optional[int] = None,
        cache_data: bool = True,
        **kwargs,
    ) -> Iterable:
        """
        Creates a data loader for training.
    
        Parameters:
            config (PretrainedConfig): Configuration for the model.
            freq (str): Frequency of the time series data.
            data: Input data for training.
            batch_size (int): Size of each batch.
            num_batches_per_epoch (int): Number of batches per epoch.
            shuffle_buffer_length (Optional[int]): Buffer length for shuffling the data.
            cache_data (bool): Whether to cache the data.
    
        Returns:
            Iterable: A data loader for training.
        """
        # Define input names based on configuration
        prediction_input_names = [
            "past_time_features",
            "past_values",
            "past_observed_mask",
            "future_time_features",
        ]
        if config.num_static_categorical_features > 0:
            prediction_input_names.append("static_categorical_features")
        if config.num_static_real_features > 0:
            prediction_input_names.append("static_real_features")
    
        training_input_names = prediction_input_names + [
            "future_values",
            "future_observed_mask",
        ]
    
        # Create and apply transformations
        transformation = create_transformation(freq, config)
        transformed_data = transformation.apply(data, is_train=True)
        if cache_data:
            transformed_data = Cached(transformed_data)
    
        # Initialize the instance splitter for training
        instance_splitter = create_instance_splitter(config, "train")
        stream = Cyclic(transformed_data).stream()
        training_instances = instance_splitter.apply(stream)
    
        return as_stacked_batches(
            training_instances,
            batch_size=batch_size,
            shuffle_buffer_length=shuffle_buffer_length,
            field_names=training_input_names,
            output_type=torch.tensor,
            num_batches_per_epoch=num_batches_per_epoch,
        )
    
    
    def create_backtest_dataloader(
        config: PretrainedConfig,
        freq: str,
        data,
        batch_size: int,
        **kwargs,
    ) -> Iterable:
        """
        Creates a data loader for backtesting.
    
        Parameters:
            config (PretrainedConfig): Configuration for the model.
            freq (str): Frequency of the time series data.
            data: Input data for backtesting.
            batch_size (int): Size of each batch.
    
        Returns:
            Iterable: A data loader for backtesting.
        """
        prediction_input_names = [
            "past_time_features",
            "past_values",
            "past_observed_mask",
            "future_time_features",
        ]
        if config.num_static_categorical_features > 0:
            prediction_input_names.append("static_categorical_features")
        if config.num_static_real_features > 0:
            prediction_input_names.append("static_real_features")
    
        # Create and apply transformations
        transformation = create_transformation(freq, config)
        transformed_data = transformation.apply(data)
    
        # Initialize the instance splitter for validation
        instance_sampler = create_instance_splitter(config, "validation")
        testing_instances = instance_sampler.apply(transformed_data, is_train=True)
    
        return as_stacked_batches(
            testing_instances,
            batch_size=batch_size,
            output_type=torch.tensor,
            field_names=prediction_input_names,
        )
    
    
    def create_test_dataloader(
        config: PretrainedConfig,
        freq: str,
        data,
        batch_size: int,
        **kwargs,
    ) -> Iterable:
        """
        Creates a data loader for testing.
    
        Parameters:
            config (PretrainedConfig): Configuration for the model.
            freq (str): Frequency of the time series data.
            data: Input data for testing.
            batch_size (int): Size of each batch.
    
        Returns:
            Iterable: A data loader for testing.
        """
        prediction_input_names = [
            "past_time_features",
            "past_values",
            "past_observed_mask",
            "future_time_features",
        ]
        if config.num_static_categorical_features > 0:
            prediction_input_names.append("static_categorical_features")
        if config.num_static_real_features > 0:
            prediction_input_names.append("static_real_features")
    
        # Create and apply transformations
        transformation = create_transformation(freq, config)
        transformed_data = transformation.apply(data, is_train=False)
    
        # Initialize the instance splitter for testing
        instance_sampler = create_instance_splitter(config, "test")
        testing_instances = instance_sampler.apply(transformed_data, is_train=False)
    
        return as_stacked_batches(
            testing_instances,
            batch_size=batch_size,
            output_type=torch.tensor,
            field_names=prediction_input_names,
        )
    
    
    def create_instance_splitter(
        config: PretrainedConfig,
        mode: str,
        train_sampler: Optional[InstanceSampler] = None,
        validation_sampler: Optional[InstanceSampler] = None,
    ) -> Transformation:
        """
        Creates an instance splitter for different modes (train, validation, test).
    
        Parameters:
            config (PretrainedConfig): Configuration for the model.
            mode (str): Mode for the instance splitter ("train", "validation", "test").
            train_sampler (Optional[InstanceSampler]): Custom sampler for training.
            validation_sampler (Optional[InstanceSampler]): Custom sampler for validation.
    
        Returns:
            Transformation: An instance splitter transformation.
        """
        assert mode in ["train", "validation", "test"], "Invalid mode specified"
    
        instance_sampler = {
            "train": train_sampler or ExpectedNumInstanceSampler(
                num_instances=1.0, min_future=config.prediction_length
            ),
            "validation": validation_sampler or ValidationSplitSampler(min_future=config.prediction_length),
            "test": TestSplitSampler(),
        }[mode]
    
        return InstanceSplitter(
            target_field="values",
            is_pad_field=FieldName.IS_PAD,
            start_field=FieldName.START,
            forecast_start_field=FieldName.FORECAST_START,
            instance_sampler=instance_sampler,
            past_length=config.context_length + max(config.lags_sequence),
            future_length=config.prediction_length,
            time_series_fields=["time_features", "observed_mask"],
        )
    
    
    def create_transformation(freq: str, config: PretrainedConfig) -> Transformation:
        """
        Creates a transformation pipeline for data preprocessing.
    
        Parameters:
            freq (str): Frequency of the time series data.
            config (PretrainedConfig): Configuration for the model.
    
        Returns:
            Transformation: A transformation pipeline.
        """
        remove_field_names = []
        if config.num_static_real_features == 0:
            remove_field_names.append(FieldName.FEAT_STATIC_REAL)
        if config.num_dynamic_real_features == 0:
            remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)
        if config.num_static_categorical_features == 0:
            remove_field_names.append(FieldName.FEAT_STATIC_CAT)
    
        return Chain(
            [
                RemoveFields(field_names=remove_field_names),
            ]
            + (
                [AsNumpyArray(
                    field=FieldName.FEAT_STATIC_CAT,
                    expected_ndim=1,
                    dtype=int,
                )]
                if config.num_static_categorical_features > 0
                else []
            )
            + (
                [AsNumpyArray(
                    field=FieldName.FEAT_STATIC_REAL,
                    expected_ndim=1,
                )]
                if config.num_static_real_features > 0
                else []
            )
            + [
                AsNumpyArray(
                    field=FieldName.TARGET,
                    expected_ndim=1 if config.input_size == 1 else 2,
                ),
                AddObservedValuesIndicator(
                    target_field=FieldName.TARGET,
                    output_field=FieldName.OBSERVED_VALUES,
                ),
                AddTimeFeatures(
                    start_field=FieldName.START,
                    target_field=FieldName.TARGET,
                    output_field=FieldName.FEAT_TIME,
                    time_features=time_features_from_frequency_str(freq),
                    pred_length=config.prediction_length,
                ),
                AddAgeFeature(
                    target_field=FieldName.TARGET,
                    output_field=FieldName.FEAT_AGE,
                    pred_length=config.prediction_length,
                    log_scale=True,
                ),
                VstackFeatures(
                    output_field=FieldName.FEAT_TIME,
                    input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]
                    + (
                        [FieldName.FEAT_DYNAMIC_REAL]
                        if config.num_dynamic_real_features > 0
                        else []
                    ),
                ),
                RenameFields(
                    mapping={
                        FieldName.FEAT_STATIC_CAT: "static_categorical_features",
                        FieldName.FEAT_STATIC_REAL: "static_real_features",
                        FieldName.FEAT_TIME: "time_features",
                        FieldName.TARGET: "values",
                        FieldName.OBSERVED_VALUES: "observed_mask",
                    }
                ),
            ]
        )
    

\end{lstlisting}

\section{Model Training Code}

The \texttt{train\_model} function is responsible for training a deep learning model using the provided training data. It utilizes the \texttt{Accelerator} from the Hugging Face \texttt{accelerate} library to manage distributed and mixed precision training, making the process more efficient.

The function accepts the number of epochs, the model to be trained, a data loader for the training data, and a title for saving outputs. It initializes the model and optimizer, and then enters a training loop where it performs forward and backward passes to update model parameters. The training loss is recorded and printed periodically.

After training, the function calculates the total training time and prints it. It also saves the training time to a text file and plots the loss history over iterations, saving the plot as an image file. This approach allows for monitoring the training process and evaluating the model's performance visually.

\begin{lstlisting}[language=Python, caption={Code for training the forecasting model}, breaklines=true, label=code5]

    # %% LIBRARIES

    from accelerate import Accelerator
    from torch.optim import AdamW
    import matplotlib.pyplot as plt
    import numpy as np
    import os
    import time
    
    def train_model(num_of_epochs, model, train_dataloader, title):
        """
        Trains a given model using the provided training data loader and saves the training loss over iterations.
        
        Parameters:
        - num_of_epochs (int): Number of epochs to train the model.
        - model (torch.nn.Module): The model to be trained.
        - train_dataloader (DataLoader): DataLoader instance for the training data.
        - title (str): Title used to name the saved plot and text file.
        
        Returns:
        - model (torch.nn.Module): The trained model.
        """
        
        epochs = num_of_epochs
        loss_history = []
    
        # Initialize Accelerator for distributed and mixed precision training
        accelerator = Accelerator()
        device = accelerator.device
    
        # Move the model to the appropriate device
        model.to(device)
        optimizer = AdamW(model.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)
    
        # Prepare model, optimizer, and dataloader for distributed training
        model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)
    
        model.train()
        
        # Record start time
        start_time = time.time()
    
        # Training loop
        for epoch in range(epochs):
            for idx, batch in enumerate(train_dataloader):
                optimizer.zero_grad()
                outputs = model(
                    static_categorical_features=batch["static_categorical_features"].to(device) if model.config.num_static_categorical_features > 0 else None,
                    static_real_features=batch["static_real_features"].to(device) if model.config.num_static_real_features > 0 else None,
                    past_time_features=batch["past_time_features"].to(device),
                    past_values=batch["past_values"].to(device),
                    future_time_features=batch["future_time_features"].to(device),
                    future_values=batch["future_values"].to(device),
                    past_observed_mask=batch["past_observed_mask"].to(device),
                    future_observed_mask=batch["future_observed_mask"].to(device),
                )
                loss = outputs.loss
    
                # Backpropagation and optimization
                accelerator.backward(loss)
                optimizer.step()
    
                loss_history.append(loss.item())
                if idx % 100 == 0:
                    print(f"Iteration {idx}: Loss = {loss.item()}")
    
        # Calculate and print the total training time
        end_time = time.time()
        training_time = end_time - start_time
        print(f"\nTraining time: {training_time:.2f} seconds\n")
    
        # Ensure the 'plots' directory exists
        plots_folder = "plots"
        if not os.path.exists(plots_folder):
            os.makedirs(plots_folder)
    
        # Save the training time to a text file
        output_file = os.path.join(plots_folder, "training_time.txt")
        with open(output_file, "w") as f:
            f.write(f"Training time for '{title}': {training_time:.2f} seconds")
    
        # Plot the loss history
        loss_history = np.array(loss_history)
        plt.figure(figsize=(10, 5))
        plt.plot(loss_history, label="Training Loss")
        plt.title("Training Loss over Iterations", fontsize=15)
        plt.xlabel("Iteration")
        plt.ylabel("Loss")
        plt.legend(loc="upper right")
    
        # Save the plot
        filename = os.path.join(plots_folder, title.replace(" ", "_") + ".png")
        plt.savefig(filename)
        print("Plot saved as:", filename)
    
        plt.show()
    
        return model
    

\end{lstlisting}

\section{Model Evaluation Code}

The provided code includes functions to generate forecasts, evaluate model performance, and visualize results for time series forecasting tasks.

\begin{itemize}
    \item \texttt{forecasting}: This function uses a trained model to generate forecasts for a given test dataloader. It moves data to the appropriate device (GPU/CPU) and accumulates forecasts from the model into a NumPy array.
    
    \item \texttt{see\_metrics}: Computes performance metrics for the forecasts, such as Mean Squared Error (MSE) and R-squared, using the `evaluate` library. It writes these metrics to a file and creates a scatter plot to visualize the relationship between MSE and R-squared values. Metrics are computed for each time series in the test dataset, and results are saved in a specified directory.
    
    \item \texttt{plot}: Generates a plot comparing the actual values of a time series against the model’s forecasts. It visualizes both the mean forecast and the uncertainty bounds (plus/minus one standard deviation). The plot is saved to a file and includes time series data with appropriate labels and formatting.
\end{itemize}

These functions facilitate model evaluation and result interpretation, providing both numerical metrics and visual insights into the forecasting performance.

\begin{lstlisting}[language=Python, caption={Code for evaluating the model’s performance}, breaklines=true, label=code6]

    # %% LIBRARIES

    import matplotlib.dates as mdates
    from evaluate import load
    import math
    from accelerate import Accelerator
    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    from gluonts.dataset.field_names import FieldName
    import os
    
    def forecasting(model, test_dataloader):
        """
        Generates forecasts using the provided model and test dataloader.
    
        Parameters:
        - model: The trained forecasting model.
        - test_dataloader: DataLoader providing the test data.
    
        Returns:
        - forecasts: Numpy array of generated forecasts.
        """
        accelerator = Accelerator()
        device = accelerator.device
        model.eval()
    
        forecasts = []
    
        for batch in test_dataloader:
            outputs = model.generate(
                static_categorical_features=batch["static_categorical_features"].to(device)
                if model.config.num_static_categorical_features > 0
                else None,
                static_real_features=batch["static_real_features"].to(device)
                if model.config.num_static_real_features > 0
                else None,
                past_time_features=batch["past_time_features"].to(device),
                past_values=batch["past_values"].to(device),
                future_time_features=batch["future_time_features"].to(device),
                past_observed_mask=batch["past_observed_mask"].to(device),
            )
            forecasts.append(outputs.sequences.cpu().numpy())
    
        forecasts = np.vstack(forecasts)
    
        return forecasts
    
    def see_metrics(forecasts, test_dataset, prediction_length, freq, output_file, title):
        """
        Computes and visualizes metrics for the forecasts and saves the results to a file.
    
        Parameters:
        - forecasts: Numpy array of forecasted values.
        - test_dataset: The test dataset containing ground truth values.
        - prediction_length: Number of time steps to predict.
        - freq: Frequency of the time series data.
        - output_file: Path to the output file where metrics will be saved.
        - title: Title for the resulting plot.
        """
        mse_metric = load("evaluate-metric/mse")
        r_squared_metric = load("evaluate-metric/r_squared")
    
        forecast_median = np.median(forecasts, 1).squeeze(0).T
    
        mse_metrics = []
        r_squared_metrics = []
    
        plots_folder = "plots"
        if not os.path.exists(plots_folder):
            os.makedirs(plots_folder)
    
        output_file = os.path.join(plots_folder, output_file)
        with open(output_file, 'w') as f:
            f.write("\t\t\tMSE\t\t\tR_squared\n")
    
            for item_id, ts in enumerate(test_dataset):
                ground_truth = ts["target"][-prediction_length:]
    
                mse = mse_metric.compute(
                    predictions=forecast_median[item_id],
                    references=np.array(ground_truth))
                mse['mse'] = 10 * math.log10(mse['mse'])
    
                r_squared = r_squared_metric.compute(
                    predictions=forecast_median[item_id],
                    references=np.array(ground_truth))
    
                mse_metrics.append(mse['mse'])
                r_squared_metrics.append(r_squared)
    
                if item_id == 0:
                    f.write(f"Temperature\t\t{mse['mse']:.6f}\t\t{r_squared:.6f}\n")
                elif item_id == 1:
                    f.write(f"Relative_humidity\t{mse['mse']:.6f}\t\t{r_squared:.6f}\n")
                elif item_id == 2:
                    f.write(f"Light\t\t\t{mse['mse']:.6f}\t\t{r_squared:.6f}\n")
                elif item_id == 3:
                    f.write(f"Soil_Temperature\t{mse['mse']:.6f}\t\t{r_squared:.6f}\n")
                elif item_id == 4:
                    f.write(f"Permittivity\t\t{mse['mse']:.6f}\t\t{r_squared:.6f}\n")
                elif item_id == 5:
                    f.write(f"Electroconductivity\t{mse['mse']:.6f}\t\t{r_squared:.6f}\n")
                elif item_id == 6:
                    f.write(f"Diameter\t\t{mse['mse']:.6f}\t\t{r_squared:.6f}\n")
    
        plt.scatter(mse_metrics, r_squared_metrics, alpha=0.2)
        plt.xlabel("MSE")
        plt.ylabel("R-squared")
    
        filename = os.path.join(plots_folder, title.replace(" ", "_") + ".png")
        plt.savefig(filename)
        print("Image saved as:", filename)
        plt.show()
    
    def plot(forecasts, ts_index, mv_index, multi_variate_test_dataset, freq, prediction_length, title):
        """
        Plots the forecasts against actual values for a specified time series.
    
        Parameters:
        - forecasts: Numpy array of forecasted values.
        - ts_index: Index of the time series to plot.
        - mv_index: Index of the multivariate time series component to plot.
        - multi_variate_test_dataset: The test dataset containing time series data.
        - freq: Frequency of the time series data.
        - prediction_length: Number of time steps to predict.
        - title: Title for the plot.
        """
        fig, ax = plt.subplots()
    
        index = pd.period_range(
            start=multi_variate_test_dataset[ts_index][FieldName.START],
            periods=len(multi_variate_test_dataset[0][FieldName.TARGET][0]),
            freq=multi_variate_test_dataset[ts_index][FieldName.START].freq,
        ).to_timestamp()
    
        ax.xaxis.set_minor_locator(mdates.HourLocator())
    
        ax.plot(
            index[-5 * prediction_length:],
            multi_variate_test_dataset[ts_index]["target"][mv_index, -5 * prediction_length:],
            label="Actual",
        )
    
        ax.plot(
            index[-prediction_length:],
            forecasts[ts_index, ..., mv_index].mean(axis=0),
            label="Mean Forecast",
        )
    
        ax.fill_between(
            index[-prediction_length:],
            forecasts[ts_index, ..., mv_index].mean(0) - forecasts[ts_index, ..., mv_index].std(axis=0),
            forecasts[ts_index, ..., mv_index].mean(0) + forecasts[ts_index, ..., mv_index].std(axis=0),
            alpha=0.2,
            interpolate=True,
            label="+/- 1-std",
        )
        ax.legend()
        ax.set_title(title.replace("_", " "))
        fig.autofmt_xdate()
    
        plots_folder = "plots"
        if not os.path.exists(plots_folder):
            os.makedirs(plots_folder)
    
        filename = os.path.join(plots_folder, title.replace(" ", "_") + ".png")
        plt.savefig(filename)
        print("Image saved as:", filename)
    
        plt.show()
    

\end{lstlisting}