% !TEX root =../LibroTipoETSI.tex
\chapter*{Resumen}
\pagestyle{especial}
\chaptermark{Resumen}
\phantomsection
\addcontentsline{toc}{listasf}{Resumen}

\lettrine[lraise=-0.1, lines=2, loversize=0.2]{E}{s}te Trabajo de Fin de Grado explora la aplicación de arquitecturas Transformer a la predicción de series temporales, examinando específicamente los modelos Transformer, Informer y Autoformer. La predicción de series temporales es un área crucial con aplicaciones en finanzas, meteorología, agricultura y más, que implica la predicción de valores futuros basándose en datos históricos.

La investigación examina la arquitectura Transformer, que se desarrolló originalmente para el procesamiento de lenguaje natural, y evalúa su potencial en la predicción de datos de series temporales. Al extender trabajos previos que emplearon enfoques estadísticos y de redes neuronales para predecir variaciones en el diámetro de plantas, este estudio busca evaluar cómo los modelos Transformer pueden mejorar las capacidades de predicción, particularmente en series de tiempo multivariadas.

El estudio comienza con una visión histórica de los métodos de predicción de series temporales, detallando la evolución desde los enfoques tradicionales hasta los modernos, y destacando los avances recientes en arquitecturas de aprendizaje automático. Luego, proporciona un examen detallado de la arquitectura Transformer, incluidos sus componentes clave y su adaptación para la predicción de series temporales. También se discuten variantes como Informer y Autoformer, con un enfoque en sus contribuciones y mejoras específicas.

La metodología de investigación incluye un análisis exhaustivo de los conjuntos de datos utilizados, los pasos de preprocesamiento realizados y el diseño experimental, que involucra la optimización de hiperparámetros y la evaluación del rendimiento. Los resultados se presentan en términos de la efectividad de los modelos basados en Transformer bajo diversas configuraciones, proporcionando información sobre su rendimiento y ventajas potenciales.

En general, este trabajo contribuye a la comprensión de los modelos Transformer en el contexto de la predicción de series temporales, ofreciendo una evaluación detallada de su eficacia y limitaciones. Los hallazgos buscan informar sobre futuras direcciones de investigación y aplicaciones prácticas, particularmente en la optimización de arquitecturas Transformer para tareas complejas de predicción.

%La hoja de estilo utilizada es una versión de la que el Prof. Payán realizó para un libro que desde hace tiempo viene escribiendo para su asignatura. Con ella se han realizado estas notas, a modo de instrucciones, añadiéndole el diseño de la portada. El diseño de la portada está basado en el que el prof. Fernando García García, de nuestra universidad, hiciera para los libros de la sección de publicación de nuestra Escuela.


\chapter*{Abstract}
\pagestyle{especial}
\chaptermark{Abstract}
\phantomsection
\addcontentsline{toc}{listasf}{Abstract}

\lettrine[lraise=-0.1, lines=2, loversize=0.2]{T}{h}is Final Degree Project explores the application of Transformer architectures to time series forecasting, specifically examining the Transformer, Informer, and Autoformer models. Time series forecasting is a crucial area with applications spanning finance, meteorology, agriculture, and more, involving the prediction of future values based on historical data.

The research investigates the Transformer architecture, which was originally developed for natural language processing, and evaluates its potential in forecasting time series data. By extending previous work that employed statistical and neural network approaches to predict plant diameter variations, this study aims to assess how Transformer models can enhance forecasting capabilities, particularly in both univariate and multivariate contexts.

The study begins with a historical overview of time series forecasting methods, detailing the evolution from traditional to modern approaches and highlighting recent advancements in machine learning architectures. It then provides an in-depth examination of the Transformer architecture, including its core components and its adaptation for time series forecasting. Variants such as Informer and Autoformer are also discussed, with a focus on their specific contributions and improvements.

The research methodology includes a comprehensive analysis of the datasets used, the preprocessing steps undertaken, and the experimental design, which involves hyperparameter tuning and performance evaluation. The results are presented in terms of the effectiveness of the Transformer-based models under various configurations, providing insights into their performance and potential advantages.

Overall, this work contributes to the understanding of Transformer models in the context of time series forecasting, offering a detailed evaluation of their efficacy and limitations. The findings aim to inform future research directions and practical applications, particularly in optimizing Transformer architectures for complex forecasting tasks.

