\chapter{Conclusions}\LABCHAP{CAP7}
\pagestyle{esitscCD}

This thesis has explored the use of Transformer architectures, specifically the Vanilla Transformer, Informer, and Autoformer, for time series forecasting. Through our research and experiments, we have gained valuable insights into the strengths and limitations of these models and their relative performance when compared to more traditional methods such as Recurrent Neural Networks (RNN) and Long Short-Term Memory networks (LSTM).

Transformers, as a general architecture, exhibit several notable advantages. They are highly effective at capturing long-term dependencies within data, which is crucial for time series forecasting tasks where understanding patterns over extended periods is essential. Moreover, Transformers are scalable and versatile, allowing them to be adapted to various tasks and data types. However, these benefits come at a cost. Standard Transformer models can be computationally intensive, requiring significant memory and processing power. Additionally, they often need a large amount of data to train effectively, which can be a limitation in situations where data is scarce.

Despite these strengths, our results indicate that the Vanilla Transformer architecture does not perform as well as its more recent counterparts, the Informer and Autoformer, especially considering the trade-offs between computational efficiency and performance. The Informer, in particular, has demonstrated remarkable efficiency, making it the most efficient of the three architectures we examined. This makes it a compelling choice when computational resources are limited or when quick predictions are needed without compromising too much on accuracy.

On the other hand, the Autoformer has shown itself to be the most promising architecture due to its innovative design that specifically targets the forecasting of time series data. However, the improvements in performance were not as significant as anticipated. It is likely that with a different approach to hyperparameter tuning, better results could have been achieved. This suggests that while the Autoformer has a strong foundation, there is room for optimization and improvement in its application to real-world data.

Overall, while these Transformer-based methods did not always surpass traditional methods such as RNNs and LSTMs in our experiments, they demonstrated considerable potential and effectiveness. The ability of Transformers to capture complex, long-range dependencies within time series data suggests that with further research and development, they could potentially outperform current standard methods.

Furthermore, our exploration of multivariate time series forecasting revealed important insights regarding hyperparameter tuning. Specifically, our findings suggest that focusing on optimizing for a single variable can often lead to better performance than attempting to tune hyperparameters for multiple variables simultaneously. This highlights a trade-off between variables, where improving the predictive accuracy for one variable can negatively impact the accuracy for others. This was particularly evident for variables such as electrical conductivity and permittivity, which displayed behavior distinct from other variables, largely due to their dependence on unpredictable factors like rainfall rather than more regular daily or seasonal patterns.

In conclusion, while the Transformer architecture, including its advanced variants like the Informer and Autoformer, offers a promising direction for time series forecasting, there remains a need for further research to optimize their performance fully. This study contributes to the understanding of these models and provides a foundation for future exploration, particularly in refining hyperparameter tuning and exploring how best to leverage the unique strengths of Transformers in different forecasting contexts.

\section{Future Work}

Based on the findings of this thesis, several avenues for future research could further enhance the understanding and application of Transformer architectures for time series forecasting. These directions focus on exploring additional Transformer variants, optimizing hyperparameters more effectively, and deepening the analysis of multivariate time series forecasting.

Firstly, expanding the exploration to include more recent variants of the Transformer architecture, such as SageFormer, InParformer, Stecformer, and SAMformer, could provide valuable insights into how these models compare to the ones studied in this work. Each of these newer models introduces unique modifications and improvements over the original Transformer design, which could potentially address some of the limitations observed in our experiments. For instance, SageFormer and Stecformer are specifically designed to handle sparse data and reduce computational costs, while InParformer and SAMformer offer advanced mechanisms for parallelization and self-attention. Investigating these architectures could not only improve forecasting accuracy but also optimize resource utilization, which is crucial for practical applications.

Another promising direction for future research involves experimenting with various hyperparameter optimization methods. The performance of machine learning models, including Transformers, heavily depends on the choice of hyperparameters, which can significantly influence the model's ability to learn from data and generalize to unseen cases. In this thesis, we primarily utilized a basic approach to hyperparameter tuning, but future studies could benefit from employing more sophisticated techniques such as grid search, random search, Bayesian optimization, gradient-based optimization, and evolutionary optimization. These methods offer different strategies for navigating the hyperparameter space, potentially leading to more optimal configurations and improved model performance. For example, Bayesian optimization uses probabilistic models to predict the performance of different hyperparameter settings, which can efficiently find the best parameters with fewer evaluations compared to random or grid search.

Lastly, there is a substantial opportunity to delve deeper into multivariate time series forecasting. This involves studying how different combinations of input variables affect the forecasting results and understanding the impact of using redundant or irrelevant variables. In our current study, we observed that including certain variables, particularly those that do not have a clear relationship with the target variable, can degrade model performance. Future research could systematically investigate the effects of variable selection on model accuracy and explore techniques for identifying and excluding redundant features. This analysis is crucial, especially in domains where multivariate data can exhibit complex interdependencies, and careful selection of relevant features can lead to more accurate and reliable forecasts.
