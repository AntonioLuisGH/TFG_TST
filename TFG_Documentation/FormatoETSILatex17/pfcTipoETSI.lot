\babel@toc {english}{}\relax 
\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {5.1}{\ignorespaces Mean Squared Errors (MSE) for different Transformer models obtained by varying the prediction length and context length, sorted by model}}{51}{table.caption.85}%
\contentsline {table}{\numberline {5.2}{\ignorespaces R-squared (R²) for different Transformer models obtained by varying the prediction length and context length, sorted by model}}{52}{table.caption.86}%
\contentsline {table}{\numberline {5.3}{\ignorespaces Training times for different Transformer models obtained by varying the prediction length and context length, sorted by model and training time values}}{52}{table.caption.87}%
\contentsline {table}{\numberline {5.4}{\ignorespaces Mean Squared Errors (MSE) for different Informer models obtained by varying the prediction length and context length, sorted by model}}{52}{table.caption.88}%
\contentsline {table}{\numberline {5.5}{\ignorespaces R-squared (R²) for different Informer models obtained by varying the prediction length and context length, sorted by model}}{53}{table.caption.89}%
\contentsline {table}{\numberline {5.6}{\ignorespaces Training times for different Informer models obtained by varying the prediction length and context length, sorted by model and training time values}}{53}{table.caption.90}%
\contentsline {table}{\numberline {5.7}{\ignorespaces Mean Squared Errors (MSE) for different Autoformer models obtained by varying the prediction length and context length, sorted by model}}{53}{table.caption.91}%
\contentsline {table}{\numberline {5.8}{\ignorespaces R-squared (R²) for different Autoformer models obtained by varying the prediction length and context length, sorted by model}}{53}{table.caption.92}%
\contentsline {table}{\numberline {5.9}{\ignorespaces Training times for different Autoformer models obtained by varying the prediction length and context length, sorted by model and training time values}}{54}{table.caption.93}%
\contentsline {table}{\numberline {5.10}{\ignorespaces Mean Squared Errors (MSE) for different Transformer models obtained by varying the Lags Sequence values, sorted by model}}{54}{table.caption.94}%
\contentsline {table}{\numberline {5.11}{\ignorespaces R-squared (R²) for different Transformer models obtained by varying the Lags Sequence values, sorted by model}}{54}{table.caption.95}%
\contentsline {table}{\numberline {5.12}{\ignorespaces Training times for different Transformer models obtained by varying the Lags Sequence values, sorted by model and training time values}}{54}{table.caption.96}%
\contentsline {table}{\numberline {5.13}{\ignorespaces Mean Squared Errors (MSE) for different Informer models obtained by varying the Lags Sequence values, sorted by model}}{55}{table.caption.97}%
\contentsline {table}{\numberline {5.14}{\ignorespaces R-squared (R²) for different Informer models obtained by varying the Lags Sequence values, sorted by model}}{55}{table.caption.98}%
\contentsline {table}{\numberline {5.15}{\ignorespaces Training times for different Informer models obtained by varying the Lags Sequence values, sorted by model and training time values}}{55}{table.caption.99}%
\contentsline {table}{\numberline {5.16}{\ignorespaces Mean Squared Errors (MSE) for different Autoformer models obtained by varying the Lags Sequence values, sorted by model}}{56}{table.caption.100}%
\contentsline {table}{\numberline {5.17}{\ignorespaces R-squared (R²) for different Autoformer models obtained by varying the Lags Sequence values, sorted by model}}{56}{table.caption.101}%
\contentsline {table}{\numberline {5.18}{\ignorespaces Training times for different Autoformer models obtained by varying the Lags Sequence values, sorted by model and training time values}}{56}{table.caption.102}%
\contentsline {table}{\numberline {5.19}{\ignorespaces Mean Squared Errors (MSE) for different Transformer models obtained by varying the Dropout, sorted by model}}{56}{table.caption.103}%
\contentsline {table}{\numberline {5.20}{\ignorespaces R-squared (R²) for different Transformer models obtained by varying the Dropout, sorted by model}}{57}{table.caption.104}%
\contentsline {table}{\numberline {5.21}{\ignorespaces Training times for different Transformer models obtained by varying the Dropout, sorted by model}}{57}{table.caption.105}%
\contentsline {table}{\numberline {5.22}{\ignorespaces Mean Squared Errors (MSE) for different Informer models obtained by varying the Dropout, sorted by model}}{57}{table.caption.106}%
\contentsline {table}{\numberline {5.23}{\ignorespaces R-squared (R²) for different Informer models obtained by varying the Dropout, sorted by model}}{57}{table.caption.107}%
\contentsline {table}{\numberline {5.24}{\ignorespaces Training times for different Informer models obtained by varying the Dropout, sorted by model}}{58}{table.caption.108}%
\contentsline {table}{\numberline {5.25}{\ignorespaces Mean Squared Errors (MSE) for different Autoformer models obtained by varying the Dropout, sorted by model}}{58}{table.caption.109}%
\contentsline {table}{\numberline {5.26}{\ignorespaces R-squared (R²) for different Autoformer models obtained by varying the Dropout, sorted by model}}{58}{table.caption.110}%
\contentsline {table}{\numberline {5.27}{\ignorespaces Training times for different Autoformer models obtained by varying the Dropout, sorted by model}}{58}{table.caption.111}%
\contentsline {table}{\numberline {5.28}{\ignorespaces Mean Squared Errors (MSE) for different Transformer models obtained by varying the number of Encoder Layers, Decoder Layers, and Dimensionality of the Transformer Layers, sorted by model}}{59}{table.caption.112}%
\contentsline {table}{\numberline {5.29}{\ignorespaces R-squared (R²) for different Transformer models obtained by varying the number of Encoder Layers, Decoder Layers, and Dimensionality of the Transformer Layers, sorted by model}}{59}{table.caption.113}%
\contentsline {table}{\numberline {5.30}{\ignorespaces Training times for different Transformer models obtained by varying the number of Encoder Layers, Decoder Layers, and Dimensionality of the Transformer Layers, sorted by model and training time values}}{59}{table.caption.114}%
\contentsline {table}{\numberline {5.31}{\ignorespaces Mean Squared Errors (MSE) for different Informer models obtained by varying the number of Encoder Layers, Decoder Layers, and Dimensionality of the Transformer Layers, sorted by model}}{59}{table.caption.115}%
\contentsline {table}{\numberline {5.32}{\ignorespaces R-squared (R²) for different Informer models obtained by varying the number of Encoder Layers, Decoder Layers, and Dimensionality of the Transformer Layers, sorted by model}}{60}{table.caption.116}%
\contentsline {table}{\numberline {5.33}{\ignorespaces Training times for different Informer models obtained by varying the number of Encoder Layers, Decoder Layers, and Dimensionality of the Transformer Layers, sorted by model and training time values}}{60}{table.caption.117}%
\contentsline {table}{\numberline {5.34}{\ignorespaces Mean Squared Errors (MSE) for different Autoformer models obtained by varying the number of Encoder Layers, Decoder Layers, and Dimensionality of the Transformer Layers, sorted by model}}{60}{table.caption.118}%
\contentsline {table}{\numberline {5.35}{\ignorespaces R-squared (R²) for different Autoformer models obtained by varying the number of Encoder Layers, Decoder Layers, and Dimensionality of the Transformer Layers, sorted by model}}{60}{table.caption.119}%
\contentsline {table}{\numberline {5.36}{\ignorespaces Training times for different Autoformer models obtained by varying the number of Encoder Layers, Decoder Layers, and Dimensionality of the Transformer Layers, sorted by model and training time values}}{61}{table.caption.120}%
\contentsline {table}{\numberline {5.37}{\ignorespaces Mean Squared Errors (MSE) for different Transformer models obtained by varying the Batch Size and Number of Epochs values, sorted by model}}{61}{table.caption.121}%
\contentsline {table}{\numberline {5.38}{\ignorespaces R-squared (R²) for different Transformer models obtained by varying the Batch Size and Number of Epochs values, sorted by model}}{61}{table.caption.122}%
\contentsline {table}{\numberline {5.39}{\ignorespaces Training times for different Transformer models obtained by varying the Batch Size and Number of Epochs values, sorted by model and training time values}}{61}{table.caption.123}%
\contentsline {table}{\numberline {5.40}{\ignorespaces Mean Squared Errors (MSE) for different Informer models obtained by varying the Batch Size and Number of Epochs values, sorted by model}}{62}{table.caption.124}%
\contentsline {table}{\numberline {5.41}{\ignorespaces R-squared (R²) for different Informer models obtained by varying the Batch Size and Number of Epochs values, sorted by model}}{62}{table.caption.125}%
\contentsline {table}{\numberline {5.42}{\ignorespaces Training times for different Informer models obtained by varying the Batch Size and Number of Epochs values, sorted by model and training time values}}{62}{table.caption.126}%
\contentsline {table}{\numberline {5.43}{\ignorespaces Mean Squared Errors (MSE) for different Autoformer models obtained by varying the Batch Size and Number of Epochs values, sorted by model}}{62}{table.caption.127}%
\contentsline {table}{\numberline {5.44}{\ignorespaces R-squared (R²) for different Autoformer models obtained by varying the Batch Size and Number of Epochs values, sorted by model}}{63}{table.caption.128}%
\contentsline {table}{\numberline {5.45}{\ignorespaces Training times for different Autoformer models obtained by varying the Batch Size and Number of Epochs values, sorted by model and training time values}}{63}{table.caption.129}%
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
