\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Transformers, Informers and Autoformers.}{13}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\newlabel{chap:CAPEJ}{{2}{13}{Transformers, Informers and Autoformers}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Transformer architecture}{13}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The Transformer - model architecture}}{14}{figure.caption.18}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:FIG}{{2.1}{14}{The Transformer - model architecture}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{Encoder-Decoder Structure}{14}{subsubsection*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Self-Attention Mechanism}{15}{subsubsection*.20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Scaled Dot-Product Attention}}{15}{figure.caption.21}\protected@file@percent }
\newlabel{fig:FIG}{{2.2}{15}{Scaled Dot-Product Attention}{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Scaled Dot-Product Attention}}{16}{figure.caption.22}\protected@file@percent }
\newlabel{fig:FIG}{{2.3}{16}{Scaled Dot-Product Attention}{figure.caption.22}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Time Series Transformer}{16}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Time Series Forecasting (TSF) Problem Formulation}{16}{subsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Illustration of Iterated Multi-Step (IMS) Forecasting}}{17}{figure.caption.23}\protected@file@percent }
\newlabel{fig:FIG}{{2.4}{17}{Illustration of Iterated Multi-Step (IMS) Forecasting}{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Illustration of Direct Multi-Step (DMS) Forecasting}}{17}{figure.caption.24}\protected@file@percent }
\newlabel{fig:FIG}{{2.5}{17}{Illustration of Direct Multi-Step (DMS) Forecasting}{figure.caption.24}{}}
\citation{inproceedings}
\citation{inproceedings}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Embedding}{18}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Channel Projection}{18}{subsubsection*.25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Example of embedding of time series xi from the temporal space (left) into the pairwise space (right). In this example, a pair of time series (x1, x2) is projected into the pairwise space as a vector x12 described by p = 3 basic metrics: x12 = [d1(x1, x2), d2(x1, x2), d3(x1, x2)] T, from "Multiple Metric Learning for large margin kNN Classification of time series" \cite  {inproceedings}}}{18}{figure.caption.26}\protected@file@percent }
\newlabel{fig:FIG}{{2.6}{18}{Example of embedding of time series xi from the temporal space (left) into the pairwise space (right). In this example, a pair of time series (x1, x2) is projected into the pairwise space as a vector x12 described by p = 3 basic metrics: x12 = [d1(x1, x2), d2(x1, x2), d3(x1, x2)] T, from "Multiple Metric Learning for large margin kNN Classification of time series" \cite {inproceedings}}{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsubsection}{Fixed Position Embedding}{18}{subsubsection*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Local Timestamp Embedding}{18}{subsubsection*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Global Timestamp Embedding}{18}{subsubsection*.29}\protected@file@percent }
\citation{zhou2021informerefficienttransformerlong}
\citation{zhou2021informerefficienttransformerlong}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The input representation of a Time Series Transformer, from Informer (Zhou, et al., 2019)\cite  {zhou2021informerefficienttransformerlong}}}{19}{figure.caption.30}\protected@file@percent }
\newlabel{fig:FIG}{{2.7}{19}{The input representation of a Time Series Transformer, from Informer (Zhou, et al., 2019)\cite {zhou2021informerefficienttransformerlong}}{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Encoder}{19}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Decoder}{19}{subsection.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Training Methodology}{19}{subsection.2.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Probabilistic Forecasting}{20}{subsection.2.2.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Ilustration of Probabilistic Forecasting}}{20}{figure.caption.31}\protected@file@percent }
\newlabel{fig:FIG}{{2.8}{20}{Ilustration of Probabilistic Forecasting}{figure.caption.31}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Informer}{20}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}ProbSparse Attention}{20}{subsection.2.3.1}\protected@file@percent }
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\citation{zhou2021informerefficienttransformerlong}
\citation{zhou2021informerefficienttransformerlong}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Vanilla self attention vs ProbSparse attention, from Autoformer (Wu, Haixu, et al., 2021)\cite  {wu2022autoformerdecompositiontransformersautocorrelation}}}{21}{figure.caption.32}\protected@file@percent }
\newlabel{fig:FIG}{{2.9}{21}{Vanilla self attention vs ProbSparse attention, from Autoformer (Wu, Haixu, et al., 2021)\cite {wu2022autoformerdecompositiontransformersautocorrelation}}{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Distilling}{21}{subsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces The illustration of ProbSparse Attention, from Informer (Zhou, et al., 2019)\cite  {zhou2021informerefficienttransformerlong}}}{22}{figure.caption.33}\protected@file@percent }
\newlabel{fig:FIG}{{2.10}{22}{The illustration of ProbSparse Attention, from Informer (Zhou, et al., 2019)\cite {zhou2021informerefficienttransformerlong}}{figure.caption.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Autoformer}{22}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Decomposition Layer}{22}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Concept of Decomposition in Time Series}{22}{subsubsection*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Decomposition in Autoformer}{22}{subsubsection*.35}\protected@file@percent }
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Autoformer architecture. The encoder eliminates the long-term trend-cyclical part by series decomposition blocks (\textcolor {blue}{blue} blocks) and focuses on seasonal patterns modeling. The decoder accumulates the trend part extracted from hidden variables progressively. The past seasonal information from encoder is utilized by the encoder-decoder Auto-Correlation (center \textcolor [rgb]{0.15,0.7,0.15}{green} block in decoder), from Autoformer (Wu, Haixu, et al., 2021)\cite  {wu2022autoformerdecompositiontransformersautocorrelation}}}{23}{figure.caption.36}\protected@file@percent }
\newlabel{fig:FIG}{{2.11}{23}{Autoformer architecture. The encoder eliminates the long-term trend-cyclical part by series decomposition blocks (\textcolor {blue}{blue} blocks) and focuses on seasonal patterns modeling. The decoder accumulates the trend part extracted from hidden variables progressively. The past seasonal information from encoder is utilized by the encoder-decoder Auto-Correlation (center \textcolor [rgb]{0.15,0.7,0.15}{green} block in decoder), from Autoformer (Wu, Haixu, et al., 2021)\cite {wu2022autoformerdecompositiontransformersautocorrelation}}{figure.caption.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Attention (Autocorrelation) Mechanism}{23}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Understanding Autocorrelation}{23}{subsubsection*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Implementing Autocorrelation with FFT}{23}{subsubsection*.39}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Vanilla self attention vs Autocorrelation mechanism, from Autoformer (Wu, Haixu, et al., 2021)\cite  {wu2022autoformerdecompositiontransformersautocorrelation}}}{24}{figure.caption.38}\protected@file@percent }
\newlabel{fig:FIG}{{2.12}{24}{Vanilla self attention vs Autocorrelation mechanism, from Autoformer (Wu, Haixu, et al., 2021)\cite {wu2022autoformerdecompositiontransformersautocorrelation}}{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Attention weights computation in frequency domain using FFT, from Autoformer (Wu, Haixu, et al., 2021)\cite  {wu2022autoformerdecompositiontransformersautocorrelation}}}{24}{figure.caption.40}\protected@file@percent }
\newlabel{fig:FIG}{{2.13}{24}{Attention weights computation in frequency domain using FFT, from Autoformer (Wu, Haixu, et al., 2021)\cite {wu2022autoformerdecompositiontransformersautocorrelation}}{figure.caption.40}{}}
\@writefile{toc}{\contentsline {subsubsection}{Time Delay Aggregation}{24}{subsubsection*.41}\protected@file@percent }
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Aggregation by time delay, from Autoformer (Wu, Haixu, et al., 2021)\cite  {wu2022autoformerdecompositiontransformersautocorrelation}}}{25}{figure.caption.42}\protected@file@percent }
\newlabel{fig:FIG}{{2.14}{25}{Aggregation by time delay, from Autoformer (Wu, Haixu, et al., 2021)\cite {wu2022autoformerdecompositiontransformersautocorrelation}}{figure.caption.42}{}}
\@setckpt{3_ChapterTranformerVariants/3_ChapterTranformerVariants}{
\setcounter{page}{26}
\setcounter{equation}{0}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{14}
\setcounter{table}{0}
\setcounter{float@type}{8}
\setcounter{FBl@b}{0}
\setcounter{FRobj}{0}
\setcounter{FRsobj}{0}
\setcounter{FBcnt}{0}
\setcounter{r@tfl@t}{0}
\setcounter{parentequation}{0}
\setcounter{ttlp@side}{0}
\setcounter{problema}{0}
\setcounter{subchapter}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{DefaultLines}{2}
\setcounter{DefaultDepth}{0}
\setcounter{L@lines}{0}
\setcounter{L@depth}{0}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{su@anzahl}{0}
\setcounter{section@level}{0}
\setcounter{Item}{17}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{0}
\setcounter{lstnumber}{1}
\setcounter{mdf@globalstyle@cnt}{1}
\setcounter{mdfcountframes}{0}
\setcounter{mdf@env@i}{0}
\setcounter{mdf@env@ii}{0}
\setcounter{mdf@zref@counter}{0}
\setcounter{endNonectr}{56}
\setcounter{currNonectr}{0}
\setcounter{currteorctr}{0}
\setcounter{endteorctr}{0}
\setcounter{teor}{0}
\setcounter{currlemactr}{0}
\setcounter{endlemactr}{0}
\setcounter{lema}{0}
\setcounter{currpropctr}{0}
\setcounter{endpropctr}{0}
\setcounter{prop}{0}
\setcounter{currcoroctr}{0}
\setcounter{endcoroctr}{0}
\setcounter{coro}{0}
\setcounter{currcormctr}{0}
\setcounter{endcormctr}{0}
\setcounter{corm}{0}
\setcounter{currdefnctr}{0}
\setcounter{enddefnctr}{0}
\setcounter{defn}{0}
\setcounter{currdemoctr}{0}
\setcounter{enddemoctr}{0}
\setcounter{demo}{0}
\setcounter{currproofctr}{0}
\setcounter{endproofctr}{0}
\setcounter{proof}{0}
\setcounter{currejmpctr}{0}
\setcounter{endejmpctr}{0}
\setcounter{ejmp}{0}
\setcounter{currejmpnctr}{0}
\setcounter{endejmpnctr}{0}
\setcounter{currsolctr}{0}
\setcounter{endsolctr}{0}
\setcounter{sol}{0}
\setcounter{currSolnctr}{0}
\setcounter{endSolnctr}{0}
\setcounter{Soln}{0}
\setcounter{currsolnctr}{0}
\setcounter{endsolnctr}{0}
\setcounter{soln}{0}
\setcounter{currprobctr}{0}
\setcounter{endprobctr}{0}
\setcounter{prob}{0}
\setcounter{ltxexample}{0}
\setcounter{FBLTpage}{0}
\setcounter{lstlisting}{0}
}
