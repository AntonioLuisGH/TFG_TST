\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Transformers, Informers and Autoformers.}{15}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\newlabel{chap:CAP3}{{3}{15}{Transformers, Informers and Autoformers}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Transformer architecture}{15}{section.3.1}\protected@file@percent }
\citation{vaswani2023attention}
\citation{vaswani2023attention}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The Transformer - model architecture, from \textit  {"Attention Is All You Need"} (Vaswani, et al., 2017) \cite  {vaswani2023attention}}}{16}{figure.caption.24}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:FIG}{{3.1}{16}{The Transformer - model architecture, from \textit {"Attention Is All You Need"} (Vaswani, et al., 2017) \cite {vaswani2023attention}}{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{Encoder-Decoder Structure}{16}{subsubsection*.25}\protected@file@percent }
\citation{vaswani2023attention}
\citation{vaswani2023attention}
\@writefile{toc}{\contentsline {subsubsection}{Self-Attention Mechanism}{17}{subsubsection*.26}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Scaled Dot-Product Attention, from \textit  {"Attention Is All You Need"} (Vaswani, et al., 2017) \cite  {vaswani2023attention}}}{17}{figure.caption.27}\protected@file@percent }
\newlabel{fig:FIG}{{3.2}{17}{Scaled Dot-Product Attention, from \textit {"Attention Is All You Need"} (Vaswani, et al., 2017) \cite {vaswani2023attention}}{figure.caption.27}{}}
\citation{vaswani2023attention}
\citation{vaswani2023attention}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Multi-Head Attention, from \textit  {"Attention Is All You Need"} (Vaswani, et al., 2017) \cite  {vaswani2023attention}}}{18}{figure.caption.28}\protected@file@percent }
\newlabel{fig:FIG}{{3.3}{18}{Multi-Head Attention, from \textit {"Attention Is All You Need"} (Vaswani, et al., 2017) \cite {vaswani2023attention}}{figure.caption.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Time Series Transformer}{18}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Time Series Forecasting (TSF) Problem Formulation}{18}{subsection.3.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Illustration of Iterated Multi-Step (IMS) Forecasting}}{19}{figure.caption.29}\protected@file@percent }
\newlabel{fig:FIG}{{3.4}{19}{Illustration of Iterated Multi-Step (IMS) Forecasting}{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Illustration of Direct Multi-Step (DMS) Forecasting}}{19}{figure.caption.30}\protected@file@percent }
\newlabel{fig:FIG}{{3.5}{19}{Illustration of Direct Multi-Step (DMS) Forecasting}{figure.caption.30}{}}
\citation{inproceedings}
\citation{inproceedings}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Embedding}{20}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Channel Projection}{20}{subsubsection*.31}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Example of embedding of time series xi from the temporal space (left) into the pairwise space (right). In this example, a pair of time series (x1, x2) is projected into the pairwise space as a vector x12 described by p = 3 basic metrics: x12 = [d1(x1, x2), d2(x1, x2), d3(x1, x2)] T, from \textit  {"Multiple Metric Learning for large margin kNN Classification of time series"} \cite  {inproceedings}}}{20}{figure.caption.32}\protected@file@percent }
\newlabel{fig:FIG}{{3.6}{20}{Example of embedding of time series xi from the temporal space (left) into the pairwise space (right). In this example, a pair of time series (x1, x2) is projected into the pairwise space as a vector x12 described by p = 3 basic metrics: x12 = [d1(x1, x2), d2(x1, x2), d3(x1, x2)] T, from \textit {"Multiple Metric Learning for large margin kNN Classification of time series"} \cite {inproceedings}}{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsubsection}{Fixed Position Embedding}{20}{subsubsection*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Local Timestamp Embedding}{20}{subsubsection*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Global Timestamp Embedding}{20}{subsubsection*.35}\protected@file@percent }
\citation{zhou2021informerefficienttransformerlong}
\citation{zhou2021informerefficienttransformerlong}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces The input representation of a Time Series Transformer, from Informer (Zhou, et al., 2019)\cite  {zhou2021informerefficienttransformerlong}}}{21}{figure.caption.36}\protected@file@percent }
\newlabel{fig:FIG}{{3.7}{21}{The input representation of a Time Series Transformer, from Informer (Zhou, et al., 2019)\cite {zhou2021informerefficienttransformerlong}}{figure.caption.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Encoder}{21}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Decoder}{21}{subsection.3.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Training Methodology}{21}{subsection.3.2.5}\protected@file@percent }
\citation{raftery2014usecommunicationprobabilisticforecasts}
\citation{hyndman2014visualization}
\citation{hyndman2014visualization}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.6}Probabilistic Forecasting}{22}{subsection.3.2.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Ilustration of Probabilistic Forecasting\cite  {hyndman2014visualization}}}{22}{figure.caption.37}\protected@file@percent }
\newlabel{fig:FIG}{{3.8}{22}{Ilustration of Probabilistic Forecasting\cite {hyndman2014visualization}}{figure.caption.37}{}}
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Informer}{23}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}ProbSparse Attention}{23}{subsection.3.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Vanilla self attention vs ProbSparse attention, from Autoformer (Wu, Haixu, et al., 2021)\cite  {wu2022autoformerdecompositiontransformersautocorrelation}}}{23}{figure.caption.38}\protected@file@percent }
\newlabel{fig:FIG}{{3.9}{23}{Vanilla self attention vs ProbSparse attention, from Autoformer (Wu, Haixu, et al., 2021)\cite {wu2022autoformerdecompositiontransformersautocorrelation}}{figure.caption.38}{}}
\citation{zhou2021informerefficienttransformerlong}
\citation{zhou2021informerefficienttransformerlong}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces The illustration of ProbSparse Attention, from Informer (Zhou, et al., 2019)\cite  {zhou2021informerefficienttransformerlong}}}{24}{figure.caption.39}\protected@file@percent }
\newlabel{fig:FIG}{{3.10}{24}{The illustration of ProbSparse Attention, from Informer (Zhou, et al., 2019)\cite {zhou2021informerefficienttransformerlong}}{figure.caption.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Distilling}{24}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Autoformer}{24}{section.3.4}\protected@file@percent }
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Decomposition Layer}{25}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Decomposition in Autoformer}{25}{subsubsection*.40}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Autoformer architecture. The encoder eliminates the long-term trend-cyclical part by series decomposition blocks (\textcolor {blue}{blue} blocks) and focuses on seasonal patterns modeling. The decoder accumulates the trend part extracted from hidden variables progressively. The past seasonal information from encoder is utilized by the encoder-decoder Auto-Correlation (center \textcolor [rgb]{0.15,0.7,0.15}{green} block in decoder), from Autoformer (Wu, Haixu, et al., 2021)\cite  {wu2022autoformerdecompositiontransformersautocorrelation}}}{25}{figure.caption.41}\protected@file@percent }
\newlabel{fig:FIG}{{3.11}{25}{Autoformer architecture. The encoder eliminates the long-term trend-cyclical part by series decomposition blocks (\textcolor {blue}{blue} blocks) and focuses on seasonal patterns modeling. The decoder accumulates the trend part extracted from hidden variables progressively. The past seasonal information from encoder is utilized by the encoder-decoder Auto-Correlation (center \textcolor [rgb]{0.15,0.7,0.15}{green} block in decoder), from Autoformer (Wu, Haixu, et al., 2021)\cite {wu2022autoformerdecompositiontransformersautocorrelation}}{figure.caption.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Attention (Autocorrelation) Mechanism}{25}{subsection.3.4.2}\protected@file@percent }
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\@writefile{toc}{\contentsline {subsubsection}{Understanding Autocorrelation}{26}{subsubsection*.42}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Vanilla self attention vs Autocorrelation mechanism, from Autoformer (Wu, Haixu, et al., 2021)\cite  {wu2022autoformerdecompositiontransformersautocorrelation}}}{26}{figure.caption.43}\protected@file@percent }
\newlabel{fig:FIG}{{3.12}{26}{Vanilla self attention vs Autocorrelation mechanism, from Autoformer (Wu, Haixu, et al., 2021)\cite {wu2022autoformerdecompositiontransformersautocorrelation}}{figure.caption.43}{}}
\@writefile{toc}{\contentsline {subsubsection}{Implementing Autocorrelation with FFT}{26}{subsubsection*.44}\protected@file@percent }
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\citation{wu2022autoformerdecompositiontransformersautocorrelation}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Attention weights computation in frequency domain using FFT, from Autoformer (Wu, Haixu, et al., 2021)\cite  {wu2022autoformerdecompositiontransformersautocorrelation}}}{27}{figure.caption.45}\protected@file@percent }
\newlabel{fig:FIG}{{3.13}{27}{Attention weights computation in frequency domain using FFT, from Autoformer (Wu, Haixu, et al., 2021)\cite {wu2022autoformerdecompositiontransformersautocorrelation}}{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsubsection}{Time Delay Aggregation}{27}{subsubsection*.46}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Aggregation by time delay, from Autoformer (Wu, Haixu, et al., 2021)\cite  {wu2022autoformerdecompositiontransformersautocorrelation}}}{28}{figure.caption.47}\protected@file@percent }
\newlabel{fig:FIG}{{3.14}{28}{Aggregation by time delay, from Autoformer (Wu, Haixu, et al., 2021)\cite {wu2022autoformerdecompositiontransformersautocorrelation}}{figure.caption.47}{}}
\@setckpt{3_ChapterTranformerVariants/3_ChapterTranformerVariants}{
\setcounter{page}{29}
\setcounter{equation}{15}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{14}
\setcounter{table}{0}
\setcounter{float@type}{8}
\setcounter{FBl@b}{0}
\setcounter{FRobj}{0}
\setcounter{FRsobj}{0}
\setcounter{FBcnt}{0}
\setcounter{r@tfl@t}{0}
\setcounter{parentequation}{0}
\setcounter{ttlp@side}{0}
\setcounter{problema}{0}
\setcounter{subchapter}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{DefaultLines}{2}
\setcounter{DefaultDepth}{0}
\setcounter{L@lines}{3}
\setcounter{L@depth}{0}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{su@anzahl}{0}
\setcounter{section@level}{0}
\setcounter{Item}{19}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{0}
\setcounter{lstnumber}{1}
\setcounter{mdf@globalstyle@cnt}{1}
\setcounter{mdfcountframes}{0}
\setcounter{mdf@env@i}{0}
\setcounter{mdf@env@ii}{0}
\setcounter{mdf@zref@counter}{0}
\setcounter{endNonectr}{59}
\setcounter{currNonectr}{0}
\setcounter{currteorctr}{0}
\setcounter{endteorctr}{0}
\setcounter{teor}{0}
\setcounter{currlemactr}{0}
\setcounter{endlemactr}{0}
\setcounter{lema}{0}
\setcounter{currpropctr}{0}
\setcounter{endpropctr}{0}
\setcounter{prop}{0}
\setcounter{currcoroctr}{0}
\setcounter{endcoroctr}{0}
\setcounter{coro}{0}
\setcounter{currcormctr}{0}
\setcounter{endcormctr}{0}
\setcounter{corm}{0}
\setcounter{currdefnctr}{0}
\setcounter{enddefnctr}{0}
\setcounter{defn}{0}
\setcounter{currdemoctr}{0}
\setcounter{enddemoctr}{0}
\setcounter{demo}{0}
\setcounter{currproofctr}{0}
\setcounter{endproofctr}{0}
\setcounter{proof}{0}
\setcounter{currejmpctr}{0}
\setcounter{endejmpctr}{0}
\setcounter{ejmp}{0}
\setcounter{currejmpnctr}{0}
\setcounter{endejmpnctr}{0}
\setcounter{currsolctr}{0}
\setcounter{endsolctr}{0}
\setcounter{sol}{0}
\setcounter{currSolnctr}{0}
\setcounter{endSolnctr}{0}
\setcounter{Soln}{0}
\setcounter{currsolnctr}{0}
\setcounter{endsolnctr}{0}
\setcounter{soln}{0}
\setcounter{currprobctr}{0}
\setcounter{endprobctr}{0}
\setcounter{prob}{0}
\setcounter{ltxexample}{0}
\setcounter{FBLTpage}{0}
\setcounter{lstlisting}{0}
}
