% !TEX root =../LibroTipoETSI.tex
\chapter{Time series Transformer}\LABCHAP{CAPEJ}
\pagestyle{esitscCD}

\section{Overview}

\subsection{Enhancing Time Series Models with Transformers}

Transformers, through their use of multi-head attention, have the potential to significantly enhance time series models' ability to manage long-term dependencies. This approach offers distinct advantages over current methodologies. To illustrate the effectiveness of transformers in handling long-term dependencies, consider how ChatGPT generates extensive and detailed responses in language models. By applying multi-head attention to time series, similar benefits can be achieved: one attention head can concentrate on long-term dependencies, while another focuses on short-term dependencies. This division of attention enables a more comprehensive analysis of the data. Consequently, we believe that transformers could enable time series models to forecast up to 1,000 data points into the future, or possibly even more.

\subsection{The Challenge of Quadratic Complexity}

Transformers face a significant challenge in calculating multi-head self-attention for time series. Since each data point in the series must interact with every other data point, adding more data points exponentially increases the computation time required. This phenomenon, known as quadratic complexity, results in a computational bottleneck when processing lengthy sequences.


