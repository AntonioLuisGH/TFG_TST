\babel@toc {english}{}\relax 
\babel@toc {english}{}\relax 
\contentsline {listasf}{√çndice Abreviado}{I}{section*.1}%
\contentsline {chapter}{\numberline {1}History and State of the Art}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Evolution of Methods for Time Series Forecasting}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Traditional Methods}{1}{subsection.1.1.1}%
\contentsline {subsubsection}{Autoregressive and Moving Average Models (AR, MA, ARMA)}{1}{subsubsection*.5}%
\contentsline {subsubsection}{Autoregressive Integrated Moving Average (ARIMA) and SARIMA Models}{2}{subsubsection*.6}%
\contentsline {subsubsection}{SARIMAX models}{3}{subsubsection*.7}%
\contentsline {subsubsection}{VARMA and VARMAX Models}{3}{subsubsection*.8}%
\contentsline {subsubsection}{Simple Exponential Smoothing (SES) and Holt-Winters Exponential Smoothing (HWES)}{4}{subsubsection*.9}%
\contentsline {subsubsection}{ARCH and GARCH Models}{5}{subsubsection*.10}%
\contentsline {subsection}{\numberline {1.1.2}Modern Methods}{6}{subsection.1.1.2}%
\contentsline {subsubsection}{Recurrent Neural Networks (RNNs) and LSTMs}{6}{subsubsection*.11}%
\contentsline {subsubsection}{Residual Networks (ResNet) and Fully Convolutional Networks (FCN)}{6}{subsubsection*.12}%
\contentsline {subsubsection}{ROCKET}{7}{subsubsection*.13}%
\contentsline {subsubsection}{Matrix Factorization with DTW}{7}{subsubsection*.14}%
\contentsline {subsubsection}{XGBoost}{7}{subsubsection*.15}%
\contentsline {subsubsection}{Prophet}{8}{subsubsection*.16}%
\contentsline {subsubsection}{TS-Chief and HIVE-COTE}{8}{subsubsection*.17}%
\contentsline {section}{\numberline {1.2}Evolution of Transformers as a Method for Time Series Forecasting}{8}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Introduction of Transformers (2017)}{9}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Application to Time Series (2019)}{9}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Enhancements and New Variants (2020)}{9}{subsection.1.2.3}%
\contentsline {subsection}{\numberline {1.2.4}Further Innovations (2021 - 2022)}{10}{subsection.1.2.4}%
\contentsline {subsection}{\numberline {1.2.5}Recent Advances (2023 - 2024)}{10}{subsection.1.2.5}%
\contentsline {chapter}{\numberline {2}Transformers, Informers and Autoformers.}{13}{chapter.2}%
\contentsline {section}{\numberline {2.1}Transformer architecture}{13}{section.2.1}%
\contentsline {subsubsection}{Encoder-Decoder Structure}{14}{subsubsection*.19}%
\contentsline {subsubsection}{Self-Attention Mechanism}{15}{subsubsection*.20}%
\contentsline {section}{\numberline {2.2}Time Series Transformer}{16}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Time Series Forecasting (TSF) Problem Formulation}{16}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Embedding}{18}{subsection.2.2.2}%
\contentsline {subsubsection}{Channel Projection}{18}{subsubsection*.25}%
\contentsline {subsubsection}{Fixed Position Embedding}{18}{subsubsection*.27}%
\contentsline {subsubsection}{Local Timestamp Embedding}{18}{subsubsection*.28}%
\contentsline {subsubsection}{Global Timestamp Embedding}{18}{subsubsection*.29}%
\contentsline {subsection}{\numberline {2.2.3}Encoder}{19}{subsection.2.2.3}%
\contentsline {subsection}{\numberline {2.2.4}Decoder}{19}{subsection.2.2.4}%
\contentsline {subsection}{\numberline {2.2.5}Training Methodology}{19}{subsection.2.2.5}%
\contentsline {subsection}{\numberline {2.2.6}Probabilistic Forecasting}{20}{subsection.2.2.6}%
\contentsline {section}{\numberline {2.3}Informer}{21}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}ProbSparse Attention}{21}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Distilling}{22}{subsection.2.3.2}%
\contentsline {section}{\numberline {2.4}Autoformer}{22}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Decomposition Layer}{23}{subsection.2.4.1}%
\contentsline {subsubsection}{Concept of Decomposition in Time Series}{23}{subsubsection*.34}%
\contentsline {subsubsection}{Decomposition in Autoformer}{23}{subsubsection*.35}%
\contentsline {subsection}{\numberline {2.4.2}Attention (Autocorrelation) Mechanism}{23}{subsection.2.4.2}%
\contentsline {subsubsection}{Understanding Autocorrelation}{24}{subsubsection*.37}%
\contentsline {subsubsection}{Implementing Autocorrelation with FFT}{24}{subsubsection*.39}%
\contentsline {subsubsection}{Time Delay Aggregation}{25}{subsubsection*.41}%
\contentsline {chapter}{\numberline {3}Materials and Methods}{27}{chapter.3}%
\contentsline {section}{\numberline {3.1}Datasets}{27}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}ORNAVERA Data Collection Device (DCD)}{27}{subsection.3.1.1}%
\contentsline {section}{\numberline {3.2}Available Variables}{28}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Temperature (t1)}{29}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Relative Humidity (rh1)}{29}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}Vapor Pressure Deficit (vpd1)}{29}{subsection.3.2.3}%
\contentsline {subsection}{\numberline {3.2.4}Light Intensity (lux)}{29}{subsection.3.2.4}%
\contentsline {subsection}{\numberline {3.2.5}Soil Temperature (st1)}{30}{subsection.3.2.5}%
\contentsline {subsection}{\numberline {3.2.6}Permittivity (p1)}{30}{subsection.3.2.6}%
\contentsline {subsection}{\numberline {3.2.7}Electrical Conductivity (ec1)}{30}{subsection.3.2.7}%
\contentsline {subsection}{\numberline {3.2.8}Volumetric Water Content (vwc1)}{31}{subsection.3.2.8}%
\contentsline {subsection}{\numberline {3.2.9}Diameter (diam1)}{31}{subsection.3.2.9}%
\contentsline {subsection}{\numberline {3.2.10}Photosynthetically Active Radiation (par)}{31}{subsection.3.2.10}%
\contentsline {section}{\numberline {3.3}Variable selection}{32}{section.3.3}%
\contentsline {section}{\numberline {3.4}Software Implementation}{33}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Anaconda and Spyder}{33}{subsection.3.4.1}%
\contentsline {subsubsection}{Anaconda}{33}{subsubsection*.58}%
\contentsline {subsubsection}{Spyder}{33}{subsubsection*.59}%
\contentsline {subsection}{\numberline {3.4.2}Google Colab}{34}{subsection.3.4.2}%
\contentsline {subsubsection}{T4 Hosted Runtime Environment}{34}{subsubsection*.60}%
\contentsline {section}{\numberline {3.5}Python Libraries}{34}{section.3.5}%
\contentsline {chapter}{\numberline {4}Experimental Design}{37}{chapter.4}%
\contentsline {section}{\numberline {4.1}Dataset Preprocessing}{37}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Structure}{37}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Signal smoothing}{38}{subsection.4.1.2}%
\contentsline {subsection}{\numberline {4.1.3}Elimination of the Trend}{39}{subsection.4.1.3}%
\contentsline {subsection}{\numberline {4.1.4}Elimination of NaN Values}{42}{subsection.4.1.4}%
\contentsline {subsection}{\numberline {4.1.5}Normalization of data}{42}{subsection.4.1.5}%
\contentsline {subsection}{\numberline {4.1.6}Transforming Irregular Time Series to Regular Time Series}{43}{subsection.4.1.6}%
\contentsline {section}{\numberline {4.2}Error Metrics}{44}{section.4.2}%
\contentsline {section}{\numberline {4.3}Hyperparameter Tuning}{46}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Prediction Length and Context Length}{46}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Lags Sequence}{46}{subsection.4.3.2}%
\contentsline {subsection}{\numberline {4.3.3}Dropout}{47}{subsection.4.3.3}%
\contentsline {subsection}{\numberline {4.3.4}Encoder Layers, Decoder Layers, and Dimensionality of the Transformer Layers}{48}{subsection.4.3.4}%
\contentsline {subsection}{\numberline {4.3.5}Batch Size and Number of Epochs}{49}{subsection.4.3.5}%
\contentsline {chapter}{\numberline {5}Experimental Results}{51}{chapter.5}%
\contentsline {section}{\numberline {5.1}Results Changing the Prediction Length and Context Length}{51}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Transformer}{51}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Informer}{52}{subsection.5.1.2}%
\contentsline {subsection}{\numberline {5.1.3}Autoformer}{52}{subsection.5.1.3}%
\contentsline {section}{\numberline {5.2}Results changing the Lags Sequence}{53}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Transformer}{54}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}Informer}{55}{subsection.5.2.2}%
\contentsline {subsection}{\numberline {5.2.3}Autoformer}{55}{subsection.5.2.3}%
\contentsline {section}{\numberline {5.3}Results changing the Dropout}{55}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Transformer}{56}{subsection.5.3.1}%
\contentsline {subsection}{\numberline {5.3.2}Informer}{56}{subsection.5.3.2}%
\contentsline {subsection}{\numberline {5.3.3}Autoformer}{57}{subsection.5.3.3}%
\contentsline {section}{\numberline {5.4}Results changing the Encoder Layers, Decoder Layers, and Dimensionality of the Transformer Layers}{57}{section.5.4}%
\contentsline {subsection}{\numberline {5.4.1}Transformer}{57}{subsection.5.4.1}%
\contentsline {subsection}{\numberline {5.4.2}Informer}{58}{subsection.5.4.2}%
\contentsline {subsection}{\numberline {5.4.3}Autoformer}{58}{subsection.5.4.3}%
\contentsline {section}{\numberline {5.5}Results changing the Batch Size and Number of Epochs}{59}{section.5.5}%
\contentsline {subsection}{\numberline {5.5.1}Transformer}{60}{subsection.5.5.1}%
\contentsline {subsection}{\numberline {5.5.2}Informer}{61}{subsection.5.5.2}%
\contentsline {subsection}{\numberline {5.5.3}Autoformer}{62}{subsection.5.5.3}%
\mbox {}\hspace *{0pt}\par 
\contentsline {listasb}{List of Figures}{65}{section*.130}%
\contentsline {listasb}{List of Tables}{67}{section*.132}%
\contentsline {listasb}{List of Codes}{69}{section*.134}%
\contentsline {listasb}{Bibliography}{71}{section*.136}%
\contentsline {listasb}{Index}{73}{section*.138}%
\contentsline {listasb}{Glossary}{73}{section*.139}%
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
