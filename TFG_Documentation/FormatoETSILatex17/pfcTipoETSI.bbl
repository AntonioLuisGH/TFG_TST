% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{FernandezYepes2022}
P.~Fernández~Yepes, ``Modelado de crecimiento de plantas con técnicas estadísticas,'' Seville, 2022.

\bibitem{BenitezGonzalez2023}
A.~Benítez~González, ``Técnicas de aprendizaje profundo para la predicción de series temporales de monitoreo de cultivos agrícolas,'' Seville, 2023.

\bibitem{LSTM}
S.~Hochreiter and J.~Schmidhuber, ``Long short-term memory,'' \emph{Neural computation}, vol.~9, pp. 1735--80, 12 1997.

\bibitem{long2015fullyconvolutionalnetworkssemantic}
\BIBentryALTinterwordspacing
J.~Long, E.~Shelhamer, and T.~Darrell, ``Fully convolutional networks for semantic segmentation,'' 2015. [Online]. Available: \url{https://arxiv.org/abs/1411.4038}
\BIBentrySTDinterwordspacing

\bibitem{Dempster_2020}
\BIBentryALTinterwordspacing
A.~Dempster, F.~Petitjean, and G.~I. Webb, ``Rocket: exceptionally fast and accurate time series classification using random convolutional kernels,'' \emph{Data Mining and Knowledge Discovery}, vol.~34, no.~5, p. 1454–1495, Jul. 2020. [Online]. Available: \url{http://dx.doi.org/10.1007/s10618-020-00701-z}
\BIBentrySTDinterwordspacing

\bibitem{bringmann2023dynamicdynamictimewarping}
\BIBentryALTinterwordspacing
K.~Bringmann, N.~Fischer, I.~van~der Hoog, E.~Kipouridis, T.~Kociumaka, and E.~Rotenberg, ``Dynamic dynamic time warping,'' 2023. [Online]. Available: \url{https://arxiv.org/abs/2310.18128}
\BIBentrySTDinterwordspacing

\bibitem{Chen_2016}
\BIBentryALTinterwordspacing
T.~Chen and C.~Guestrin, ``Xgboost: A scalable tree boosting system,'' in \emph{Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, ser. KDD ’16.\hskip 1em plus 0.5em minus 0.4em\relax ACM, Aug. 2016. [Online]. Available: \url{http://dx.doi.org/10.1145/2939672.2939785}
\BIBentrySTDinterwordspacing

\bibitem{prophet}
S.~J. Taylor and B.~Letham, ``Prophet: Forecasting at scale,'' \url{https://facebook.github.io/prophet/}, 2017, accedido: 14 de julio de 2024.

\bibitem{Shifaz_2020}
\BIBentryALTinterwordspacing
A.~Shifaz, C.~Pelletier, F.~Petitjean, and G.~I. Webb, ``Ts-chief: a scalable and accurate forest algorithm for time series classification,'' \emph{Data Mining and Knowledge Discovery}, vol.~34, no.~3, p. 742–775, Mar. 2020. [Online]. Available: \url{http://dx.doi.org/10.1007/s10618-020-00679-8}
\BIBentrySTDinterwordspacing

\bibitem{7837946}
J.~Lines, S.~Taylor, and A.~Bagnall, ``Hive-cote: The hierarchical vote collective of transformation-based ensembles for time series classification,'' in \emph{2016 IEEE 16th International Conference on Data Mining (ICDM)}, 2016, pp. 1041--1046.

\bibitem{vaswani2023attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, L.~Kaiser, and I.~Polosukhin, \emph{Attention Is All You Need}, 2023.

\bibitem{zeng2022transformerseffectivetimeseries}
\BIBentryALTinterwordspacing
A.~Zeng, M.~Chen, L.~Zhang, and Q.~Xu, ``Are transformers effective for time series forecasting?'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2205.13504}
\BIBentrySTDinterwordspacing

\bibitem{wen2023transformerstimeseriessurvey}
\BIBentryALTinterwordspacing
Q.~Wen, T.~Zhou, C.~Zhang, W.~Chen, Z.~Ma, J.~Yan, and L.~Sun, ``Transformers in time series: A survey,'' 2023. [Online]. Available: \url{https://arxiv.org/abs/2202.07125}
\BIBentrySTDinterwordspacing

\bibitem{lim2020temporalfusiontransformersinterpretable}
\BIBentryALTinterwordspacing
B.~Lim, S.~O. Arik, N.~Loeff, and T.~Pfister, ``Temporal fusion transformers for interpretable multi-horizon time series forecasting,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/1912.09363}
\BIBentrySTDinterwordspacing

\bibitem{zhou2021informerefficienttransformerlong}
\BIBentryALTinterwordspacing
H.~Zhou, S.~Zhang, J.~Peng, S.~Zhang, J.~Li, H.~Xiong, and W.~Zhang, ``Informer: Beyond efficient transformer for long sequence time-series forecasting,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2012.07436}
\BIBentrySTDinterwordspacing

\bibitem{li2020enhancinglocalitybreakingmemory}
\BIBentryALTinterwordspacing
S.~Li, X.~Jin, Y.~Xuan, X.~Zhou, W.~Chen, Y.-X. Wang, and X.~Yan, ``Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/1907.00235}
\BIBentrySTDinterwordspacing

\bibitem{kitaev2020reformerefficienttransformer}
\BIBentryALTinterwordspacing
N.~Kitaev, Łukasz Kaiser, and A.~Levskaya, ``Reformer: The efficient transformer,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/2001.04451}
\BIBentrySTDinterwordspacing

\bibitem{wu2022autoformerdecompositiontransformersautocorrelation}
\BIBentryALTinterwordspacing
H.~Wu, J.~Xu, J.~Wang, and M.~Long, ``Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2106.13008}
\BIBentrySTDinterwordspacing

\bibitem{zhou2022fedformerfrequencyenhanceddecomposed}
\BIBentryALTinterwordspacing
T.~Zhou, Z.~Ma, Q.~Wen, X.~Wang, L.~Sun, and R.~Jin, ``Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2201.12740}
\BIBentrySTDinterwordspacing

\bibitem{liu2022pyraformer}
\BIBentryALTinterwordspacing
S.~Liu, H.~Yu, C.~Liao, J.~Li, W.~Lin, A.~X. Liu, and S.~Dustdar, ``Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting,'' in \emph{International Conference on Learning Representations}, 2022. [Online]. Available: \url{https://openreview.net/forum?id=0EXmFzUn5I}
\BIBentrySTDinterwordspacing

\bibitem{woo2022etsformerexponentialsmoothingtransformers}
\BIBentryALTinterwordspacing
G.~Woo, C.~Liu, D.~Sahoo, A.~Kumar, and S.~Hoi, ``Etsformer: Exponential smoothing transformers for time-series forecasting,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2202.01381}
\BIBentrySTDinterwordspacing

\bibitem{wang2021crossformerversatilevisiontransformer}
\BIBentryALTinterwordspacing
W.~Wang, L.~Yao, L.~Chen, B.~Lin, D.~Cai, X.~He, and W.~Liu, ``Crossformer: A versatile vision transformer hinging on cross-scale attention,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2108.00154}
\BIBentrySTDinterwordspacing

\bibitem{nie2023timeseriesworth64}
\BIBentryALTinterwordspacing
Y.~Nie, N.~H. Nguyen, P.~Sinthong, and J.~Kalagnanam, ``A time series is worth 64 words: Long-term forecasting with transformers,'' 2023. [Online]. Available: \url{https://arxiv.org/abs/2211.14730}
\BIBentrySTDinterwordspacing

\bibitem{xue2024cardchannelalignedrobust}
\BIBentryALTinterwordspacing
W.~Xue, T.~Zhou, Q.~Wen, J.~Gao, B.~Ding, and R.~Jin, ``Card: Channel aligned robust blend transformer for time series forecasting,'' 2024. [Online]. Available: \url{https://arxiv.org/abs/2305.12095}
\BIBentrySTDinterwordspacing

\bibitem{zhang2023sageformerseriesawareframeworklongterm}
\BIBentryALTinterwordspacing
Z.~Zhang, L.~Meng, and Y.~Gu, ``Sageformer: Series-aware framework for long-term multivariate time series forecasting,'' 2023. [Online]. Available: \url{https://arxiv.org/abs/2307.01616}
\BIBentrySTDinterwordspacing

\bibitem{Cao_Huang_Yao_Wang_He_Wang_2023}
\BIBentryALTinterwordspacing
H.~Cao, Z.~Huang, T.~Yao, J.~Wang, H.~He, and Y.~Wang, ``Inparformer: Evolutionary decomposition transformers with interactive parallel attention for long-term time series forecasting,'' \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, vol.~37, no.~6, pp. 6906--6915, Jun. 2023. [Online]. Available: \url{https://ojs.aaai.org/index.php/AAAI/article/view/25845}
\BIBentrySTDinterwordspacing

\bibitem{sun2023stecformerspatiotemporalencodingcascaded}
\BIBentryALTinterwordspacing
Z.~Sun, Y.~Wei, W.~Jia, and L.~Yu, ``Stecformer: Spatio-temporal encoding cascaded transformer for multivariate long-term time series forecasting,'' 2023. [Online]. Available: \url{https://arxiv.org/abs/2305.16370}
\BIBentrySTDinterwordspacing

\bibitem{ilbert2024samformerunlockingpotentialtransformers}
\BIBentryALTinterwordspacing
R.~Ilbert, A.~Odonnat, V.~Feofanov, A.~Virmaux, G.~Paolo, T.~Palpanas, and I.~Redko, ``Samformer: Unlocking the potential of transformers in time series forecasting with sharpness-aware minimization and channel-wise attention,'' 2024. [Online]. Available: \url{https://arxiv.org/abs/2402.10198}
\BIBentrySTDinterwordspacing

\bibitem{inproceedings}
C.~T. Do, A.~Douzal, S.~Marie, and M.~Rombaut, ``Multiple metric learning for large margin knn classification of time series,'' 09 2015.

\bibitem{raftery2014usecommunicationprobabilisticforecasts}
\BIBentryALTinterwordspacing
A.~E. Raftery, ``Use and communication of probabilistic forecasts,'' 2014. [Online]. Available: \url{https://arxiv.org/abs/1408.4812}
\BIBentrySTDinterwordspacing

\bibitem{hyndman2014visualization}
\BIBentryALTinterwordspacing
R.~J. Hyndman, ``Visualization of probabilistic forecasts,'' \emph{Hyndsight}, 2014. [Online]. Available: \url{https://robjhyndman.com/hyndsight/visualization-of-probabilistic-forecasts/index.html}
\BIBentrySTDinterwordspacing

\bibitem{ornavera2020dcd}
\BIBentryALTinterwordspacing
Ornavera, ``Ornavera data collection device (dcd) datasheet,'' 2020, accessed: 2024-07-26. [Online]. Available: \url{https://ornavera.com/wp-content/uploads/dcd-ornavera.pdf}
\BIBentrySTDinterwordspacing

\bibitem{sensirion2020sht20}
\BIBentryALTinterwordspacing
Sensirion, ``Sht20 humidity and temperature sensor,'' 2020. [Online]. Available: \url{https://sensirion.com/products/catalog/SHT20}
\BIBentrySTDinterwordspacing

\bibitem{endeavour2020mec10}
\BIBentryALTinterwordspacing
D.~E.~T. Co., ``Mec-10 soil moisture, ec, and temperature sensor,'' 2020. [Online]. Available: \url{https://www.infwin.com/mec10-soil-moisture-ec-and-temperature-sensor-rs485-modbus/}
\BIBentrySTDinterwordspacing

\bibitem{vishay2020veml7700}
\BIBentryALTinterwordspacing
Vishay, ``Veml7700 high accuracy ambient light sensor,'' 2020. [Online]. Available: \url{https://www.vishay.com/docs/84286/veml7700.pdf}
\BIBentrySTDinterwordspacing

\bibitem{nvidia_t4_image}
N.~Corporation, ``Nvidia t4 tensor core gpu,'' \url{https://www.nvidia.com/es-es/data-center/tesla-t4/}, 2024, imagen de la GPU NVIDIA T4.

\end{thebibliography}
